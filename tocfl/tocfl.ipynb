{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e00e17d9-146c-494f-afb1-33aefe977942",
   "metadata": {},
   "source": [
    "# TOCFL/CCCC wordlists parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e8c87a-efb1-4211-890e-cb26b3ff69ab",
   "metadata": {},
   "source": [
    "Parse wordlists from https://tocfl.edu.tw/index.php/exam/download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d5ce736-1544-4cf1-9d41-669e45aa1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q opencc genanki\n",
    "\n",
    "import os, re, glob, requests, io, urllib, json, shutil\n",
    "import pandas as pd\n",
    "import opencc\n",
    "import genanki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5794d3e7-07ce-468e-bdd7-6714600c9b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path to TOCFL .xlsx to parse or comment out to download latest automatically\n",
    "TOCFL_XLS=\"downloads/8000zhuyin_202307.xlsx\"\n",
    "#TOCFL_XLS=\"downloads/8000zhuyin_202204.xlsx\"\n",
    "#TOCFL_XLS=\"downloads/8000zhuyin_20180419.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7095510-afa9-40be-b879-f8d4caf8da2b",
   "metadata": {},
   "source": [
    "### Download latest .xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f4a9ebe-d428-45e1-b09a-b11e774f7ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest file automatically from https://tocfl.edu.tw/index.php/exam/download\n",
    "if 'TOCFL_XLS' not in globals() or not os.path.exists(TOCFL_XLS):\n",
    "    TOCFL_HOME = 'https://tocfl.edu.tw/index.php/exam/download'\n",
    "    print(f'Downloading {TOCFL_HOME}')\n",
    "    resp = requests.get(TOCFL_HOME).content.decode('utf-8')\n",
    "\n",
    "    urls = re.findall('<a href=\"(/assets/files/vocabulary/8000zhuyin_[0-9]+.zip)\"', resp)\n",
    "    assert len(urls) == 1\n",
    "    TOCFL_URL = urllib.parse.urljoin(TOCFL_HOME, urls[0])\n",
    "    TOCFL_XLS = 'downloads/' + os.path.basename(TOCFL_URL).replace('.zip', '.xlsx')\n",
    "\n",
    "    ![[ ! -d downloads && -d ../downloads/tocfl ]] && ln -s ../downloads/tocfl downloads\n",
    "    !mkdir -p downloads\n",
    "    if not os.path.exists(f\"downloads/{os.path.basename(TOCFL_URL)}\"):\n",
    "        !echo \"Downloading {TOCFL_URL}\"\n",
    "        !cd downloads && wget -nc \"{TOCFL_URL}\"\n",
    "    !rm -rf downloads/unpacked && mkdir downloads/unpacked\n",
    "    !cd downloads/unpacked && unzip \"../{os.path.basename(TOCFL_URL)}\"\n",
    "    !cp -fv \"$(find downloads/unpacked -name '*.xlsx')\" \"{TOCFL_XLS}\"\n",
    "    !rm -rf downloads/unpacked\n",
    "    !echo; ls -l \"{TOCFL_XLS}\"; sha256sum \"{TOCFL_XLS}\"; chmod a-w \"{TOCFL_XLS}\"\n",
    "\n",
    "    print(f'\\nTOCFL_URL=\"{TOCFL_URL}\"\\nTOCFL_XLS=\"{TOCFL_XLS}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76375b0f-2383-4038-8f0f-17b5dcfad024",
   "metadata": {},
   "source": [
    "### Parse .xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4373f6e-6c4d-4f1c-be38-84316f57e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing downloads/8000zhuyin_202307.xlsx\n",
      "e979ac6d953fb493502e54536b4b6ff534d06e3938700052aa15806d514efc92  downloads/8000zhuyin_202307.xlsx\n",
      "Sheet 1: 準備級一級(Novice 1)\tL0-1\t160 rows\n",
      "Sheet 2: 準備級二級(Novice 2)\tL0-2\t234 rows\n",
      "Sheet 3: 入門級(Level 1)   \tL1\t347 rows\n",
      "Sheet 4: 基礎級(Level 2)   \tL2\t485 rows\n",
      "Sheet 5: 進階級(Level 3)   \tL3\t1173 rows\n",
      "Sheet 6: 高階級(Level 4)   \tL4\t2342 rows\n",
      "Sheet 7: 流利級(Level 5)   \tL5\t2776 rows\n",
      "Total: 7517\n",
      "\n",
      "各等詞條數(Entry Number)\n",
      "  Unnamed: 0 準備1級 準備2級  入門級   基礎級   進階級   高階級   流利級    總計\n",
      "0      各等詞彙量  160  234  347   485  1173  2342  2776  7517\n",
      "1      累計詞彙量       394  741  1226  2399  4741  7517      \n"
     ]
    }
   ],
   "source": [
    "LEVELS_EN_MP = {\n",
    "    'Novice 1': 'L0-1',\n",
    "    'Novice 2': 'L0-2',\n",
    "    'Level 1': 'L1',\n",
    "    'Level 2': 'L2',\n",
    "    'Level 3': 'L3',\n",
    "    'Level 4': 'L4',\n",
    "    'Level 5': 'L5',\n",
    "}\n",
    "LEVELS_CN_MP = {  # for 2018 file\n",
    "    '準備級一級': 'L0-1',\n",
    "    '準備級二級': 'L0-2',\n",
    "    '入門級': 'L1',\n",
    "    '基礎級': 'L2',\n",
    "    '進階級': 'L3',\n",
    "    '高階級': 'L4',\n",
    "    '流利級': 'L5',\n",
    "}\n",
    "\n",
    "print(f'Parsing {TOCFL_XLS}')\n",
    "!sha256sum {TOCFL_XLS}\n",
    "\n",
    "xls = pd.ExcelFile(TOCFL_XLS)\n",
    "sheets = {}\n",
    "\n",
    "for i, name in enumerate(xls.sheet_names):\n",
    "    df = xls.parse(name, dtype='str').fillna('')\n",
    "    if 'Entry Number' in name or '各等詞條數' in name:\n",
    "        break\n",
    "\n",
    "    if name in LEVELS_CN_MP:\n",
    "        level = LEVELS_CN_MP[name]\n",
    "    else:\n",
    "        level = LEVELS_EN_MP[re.findall('[(](.*)[)]', name)[0]]\n",
    "    print(f'Sheet {i+1}: {name:<15}\\t{level}\\t{len(df)} rows')\n",
    "\n",
    "    df = df.rename(columns=lambda s: s.strip().split('\\n')[-1])\n",
    "    df = df.rename(columns={\n",
    "        'Parts of Speech': 'POS',\n",
    "        '詞彙': 'Vocabulary',\n",
    "        '漢語拼音': 'Pinyin',\n",
    "        '注音': 'Zhuyin',\n",
    "        '詞類': 'POS',\n",
    "    })\n",
    "    df['Level'] = level\n",
    "    df = df[df.Vocabulary.fillna('') != ''].copy()\n",
    "    if level.startswith('L0'):\n",
    "        df['ID'] = ['%s%.3d' % (level, i+1) for i in range(len(df))]\n",
    "    else:\n",
    "        df['ID'] = ['%s-%.4d' % (level, i+1) for i in range(len(df))]\n",
    "    sheets[level] = df\n",
    "\n",
    "excel_df = pd.concat(sheets.values())\n",
    "print(f'Total: {len(excel_df)}')\n",
    "\n",
    "print(f'\\n{name}\\n%s' % str(df))\n",
    "assert str(len(excel_df)) in str(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93782fd6-a769-4ffa-aba9-61995482a821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Context</th>\n",
       "      <th>Vocabulary</th>\n",
       "      <th>Pinyin</th>\n",
       "      <th>POS</th>\n",
       "      <th>Level</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1226</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "      <td>7517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>12</td>\n",
       "      <td>7189</td>\n",
       "      <td>6780</td>\n",
       "      <td>104</td>\n",
       "      <td>7</td>\n",
       "      <td>7517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>個人資料</td>\n",
       "      <td>中</td>\n",
       "      <td>jí</td>\n",
       "      <td>N</td>\n",
       "      <td>L5</td>\n",
       "      <td>L0-1001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>220</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2985</td>\n",
       "      <td>2776</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Context Vocabulary Pinyin   POS Level       ID\n",
       "count     1226       7517   7517  7517  7517     7517\n",
       "unique      12       7189   6780   104     7     7517\n",
       "top       個人資料          中    jí      N    L5  L0-1001\n",
       "freq       220          3      7  2985  2776        1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "excel_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d298ceaf-feaa-43ef-b6e4-6ccac9575511",
   "metadata": {},
   "source": [
    "### Cleanup and convert to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "867895c5-3e95-4e44-8fa8-4d0cb2feb0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "opencc_tw2s = opencc.OpenCC('tw2s')\n",
    "\n",
    "# Character levels from Table of General Standard Chinese Characters for verification.\n",
    "tgh_level = pd.read_csv('../chars/tgh.csv').set_index('char').level.to_dict()\n",
    "\n",
    "# Convert to simplified characters + verify\n",
    "def to_simplified(trad):\n",
    "    simp = opencc_tw2s.convert(trad)\n",
    "    for x, y in ('擡抬', '砲炮', '牠它', '艶艳', '妳你'):\n",
    "        simp = simp.replace(x, y)\n",
    "    if '/' in simp and len(set(simp.split('/'))) == 1:\n",
    "        simp = simp.split('/')[0]\n",
    "    for c in simp:\n",
    "        assert tgh_level.get(c, 9) <= 2 or c in '/(),', (trad, simp, c, tgh_level.get(c))\n",
    "    return simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf860496-b44d-4ec8-a9d4-c4c6d3c3c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct some errors in pinyin, most missing apostrophes but some more serious errors too (at the front)\n",
    "pinyin_corr_df = pd.read_csv('data/errata-pinyin.csv', comment='#', dtype='str')\n",
    "\n",
    "def fix_pinyin(s, trad=''):\n",
    "    \"\"\"Cleans up text in 'Pinyin' column.\"\"\"\n",
    "\n",
    "    for x, y in ['ăǎ', 'ŏǒ', 'ĭǐ', 'ŭǔ', 'ɑa', '；/', '（(', '）)', \n",
    "                 ('\\u200b', ''), (' +[)]', ')'), (' */ *', '/'), (r'\\s+', ' ')]:\n",
    "        s = re.sub(x, y, s).strip()\n",
    "    for row in pinyin_corr_df.itertuples():\n",
    "        if row.Pinyin == s and row.Traditional == trad:\n",
    "            s = row.Corrected\n",
    "    s = s.strip()\n",
    "    assert re.match('^[a-zāáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ/(), \\']+$', s.lower()), (s, repr(s))\n",
    "    return s\n",
    "\n",
    "assert fix_pinyin('xiăo') == 'xiǎo'   # a c -> a v\n",
    "assert fix_pinyin('chuāng(zi )/chuānghu') == 'chuāng(zi)/chuānghu'\n",
    "assert fix_pinyin('liànài', trad='戀愛') == \"liàn'ài\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2673a3e9-88ab-4cff-89e0-35baadfff546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_vocabulary(s):\n",
    "    \"\"\"Cleans up text in 'Vocabulary' column.\"\"\"\n",
    "\n",
    "    for x, y in [\n",
    "        ('\\u200b', ''), ('（', '('), ('）', ')'), (' */ *', '/'), (r'\\s+', ' '),\n",
    "        ('[(]面˙ㄇㄧㄢ[)]', '(面)'), ('、', ','),\n",
    "        (r'^([\\u4E00-\\u9FFF]{2,3})\\[([\\u4E00-\\u9FFF]{2,3})\\]$', r'\\1/\\2'), #CCCC simplified\n",
    "    ]:\n",
    "        s = re.sub(x, y, s).strip()\n",
    "\n",
    "    # Zhuyin hints are reduntant with pinyin and not very relevant for foreigners, drop them\n",
    "    s = re.sub(r'[(（][ㄅㄈㄉㄊㄋㄍㄎㄏㄐㄑㄒㄓㄕㄗㄙㄚㄛㄞㄌㄟㄠㄡㄢㄣㄤㄧ一ㄨㄩㄇㄝㄆㄌㄨㄥㄔㄘㄖˊˋ˙\\uf8f8]+[)）]', '', s).strip()\n",
    "\n",
    "    assert re.match('^[\\u4E00-\\u9FFF/(),]+$', s), (row, s)\n",
    "    return s\n",
    "\n",
    "assert fix_vocabulary('名字(˙ㄗ)') == '名字'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aebee411-4190-4948-b401-b8a35ea0def7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambiguous variant entries - manually disambiguated\n",
    "variants_exc_df = pd.read_csv('data/variants.csv', comment='#', dtype='str')\n",
    "variants_exc_mp = variants_exc_df.set_index(['Vocabulary', 'Pinyin', 'POS']).Variants.to_dict()\n",
    "\n",
    "# Some variants in TOCFL are specified as \"...x/y...\" character pairs. Valid pairs here:\n",
    "variant_pairs = [\n",
    "    '做作', '布佈', '嘗嚐', '溼濕', '分份', '畫劃', '裡裏', '秘祕', '台臺', '周週',\n",
    "    '汙污', '消宵', '占佔', '證証', '雇僱', '迴回', '剎煞', '的地', '艶豔', '嘆歎',\n",
    "    '連聯', '秘祕', '伙夥',\n",
    "]\n",
    "\n",
    "def get_variants(vocab, pinyin, pos) -> str:\n",
    "    \"\"\"Returns disambiguated list of variants from TOCFL's vocab+pinyin+pos strings.\"\"\"\n",
    "\n",
    "    res = variants_exc_mp.get((vocab, pinyin, pos))\n",
    "    if res:\n",
    "        return res.strip()\n",
    "\n",
    "    ps = re.sub('[^()/,]', '', pinyin)\n",
    "    vs = re.sub('[^()/,]', '', vocab)\n",
    "    if ps == '' and vs == '':\n",
    "        return ''\n",
    "\n",
    "    if ps == '' and vs == '/':\n",
    "        for x, y in variant_pairs:\n",
    "            if not (f'{x}/{y}' in vocab or f'{y}/{x}' in vocab): continue\n",
    "            if f'{y}/{x}' in vocab:\n",
    "                x, y = y, x\n",
    "            assert f'{x}/{y}' in vocab\n",
    "            vx = vocab.replace(f'{x}/{y}', x)\n",
    "            vy = vocab.replace(f'{x}/{y}', y)\n",
    "            return f'{vx} [{pinyin}] / {vy} [{pinyin}]'\n",
    "\n",
    "    if (ps != vs and ps != '' and vs != '') or \\\n",
    "       (ps+vs != '' and '/' in pos) or \\\n",
    "       (ps == '' and vs != '' and len(set(map(len, vocab.split('/')))) != 1):\n",
    "        raise Exception('Ambiguous entry: %s' % ','.join([vocab, pinyin, pos, vocab]))\n",
    "        print('%s' % ','.join([vocab, pinyin, pos, vocab]))\n",
    "        return ''\n",
    "\n",
    "    assert '/' not in pos\n",
    "    if ps != '' and vs == '':\n",
    "        assert ps == '/'\n",
    "        res = ' / '.join([f'{vocab} [{p.strip()}]' for p in pinyin.split('/')])\n",
    "        return res\n",
    "    if ps == '' and vs != '':\n",
    "        res = vocab.split('/')\n",
    "        assert len(set(map(len, res))) == 1, vocab\n",
    "        res = ' / '.join([f'{s} [{pinyin}]' for s in res])\n",
    "        return res\n",
    "    assert ps == vs\n",
    "\n",
    "    res = []\n",
    "    for vo, py in zip(vocab.split('/'), pinyin.split('/')):\n",
    "        assert vo.count('(') == py.count('(') and vo.count('(') <= 1\n",
    "        assert vo.count(')') == py.count(')') and vo.count(')') <= 1\n",
    "        assert vo.count('(') == vo.count(')')\n",
    "        if '(' in vo:\n",
    "            vm = re.match('^([^() ]*) *[(]([^() ]+)[)] *([^() ]*)$', vo)\n",
    "            assert vm, vo\n",
    "            pm = re.match('^([^() ]*) *[(]([^() ]+)[)] *([^() ]*)$', py)\n",
    "            assert pm, py\n",
    "            res.append(f'{vm[1]}{vm[2]}{vm[3]} [{pm[1]}{pm[2]}{pm[3]}]')\n",
    "            res.append(f'{vm[1]}{vm[3]} [{pm[1]}{pm[3]}]')\n",
    "        else:\n",
    "            res.append(f'{vo} [{py}]')\n",
    "    res = ' / '.join(res)\n",
    "    return res\n",
    "\n",
    "def variants_to_json(variants):\n",
    "    if not variants or variants != variants:\n",
    "        return ''\n",
    "    arr = []\n",
    "    for var in variants.split(' / '):\n",
    "        m = re.match(r'^([^ ()\\[\\]]+) \\[([^()\\[\\]]+)\\](?:$| [(]([A-Z]+)[)])$', var)\n",
    "        assert m, variants\n",
    "        var = {\n",
    "            'Traditional': m[1],\n",
    "            'Simplified': to_simplified(m[1]),\n",
    "            'Pinyin': fix_pinyin(m[2], m[1]),\n",
    "        }\n",
    "        if m[3]:\n",
    "            var['POS'] = m[3]\n",
    "        arr.append(var)\n",
    "    return json.dumps(arr, ensure_ascii=False)\n",
    "\n",
    "assert get_variants('台灣/臺灣', 'táiwān', 'N') == '台灣 [táiwān] / 臺灣 [táiwān]'\n",
    "assert get_variants('小孩(子)', 'xiăohái(zi)', 'N') == '小孩子 [xiăoháizi] / 小孩 [xiăohái]'\n",
    "assert get_variants('公共汽車/公車', 'gōnggòngqìchē/gōngchē', 'N') == '公共汽車 [gōnggòngqìchē] / 公車 [gōngchē]'\n",
    "assert get_variants('盒/盒(子)', 'hé/hézi', 'M / N') == '盒 [hé] (M) / 盒子 [hézi] (N)'\n",
    "assert get_variants('差(一)點/差(一)點兒', 'chà(yī)diǎn/chà(yī)diǎnr','Adv') == \\\n",
    "                    '差一點 [chàyīdiǎn] / 差點 [chàdiǎn] / 差一點兒 [chàyīdiǎnr] / 差點兒 [chàdiǎnr]'\n",
    "assert get_variants('角色', 'jiǎo/juésè', 'N') == '角色 [jiǎo] / 角色 [juésè]'\n",
    "assert get_variants('計畫/劃', 'jìhuà', 'V') == '計畫 [jìhuà] / 計劃 [jìhuà]'\n",
    "assert get_variants('(老)鼠', '(lǎo)shǔ', 'N') == '老鼠 [lǎoshǔ] / 鼠 [shǔ]'\n",
    "assert get_variants('鼻(子)', 'bí(zi)', 'N') == '鼻子 [bízi] / 鼻 [bí]'\n",
    "\n",
    "assert (variants_to_json('台灣 [táiwān] / 臺灣 [táiwān]') ==\n",
    "        '[{\"Traditional\": \"台灣\", \"Simplified\": \"台湾\", \"Pinyin\": \"táiwān\"}, ' +\n",
    "        '{\"Traditional\": \"臺灣\", \"Simplified\": \"台湾\", \"Pinyin\": \"táiwān\"}]')\n",
    "assert (variants_to_json('盤 [pán] (M) / 盤子 [pánzi] (N)') == \n",
    "        '[{\"Traditional\": \"盤\", \"Simplified\": \"盘\", \"Pinyin\": \"pán\", \"POS\": \"M\"}, ' +\n",
    "        '{\"Traditional\": \"盤子\", \"Simplified\": \"盘子\", \"Pinyin\": \"pánzi\", \"POS\": \"N\"}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02238ac9-fd82-47bb-b560-cb1793d68b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tocfl.csv: 7517 entries\n",
      "tocfl-expanded.csv: 7847 entries\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "expanded_rows = []\n",
    "\n",
    "for row in excel_df.fillna('').to_dict(orient='records'):\n",
    "    row['Traditional'] = fix_vocabulary(row['Vocabulary'])\n",
    "    row['Pinyin'] = fix_pinyin(row['Pinyin'], row['Traditional'])\n",
    "    row['Variants'] = variants_to_json(get_variants(row['Traditional'], row['Pinyin'], row['POS']))\n",
    "    row['POS'] = row['POS'].replace(' ', '').replace('；', '/')\n",
    "    row['Simplified'] = to_simplified(row['Traditional'])\n",
    "    rows.append(row)\n",
    "\n",
    "    variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "    for variant in variants:\n",
    "        var = dict(row)\n",
    "        var.update(variant)\n",
    "        expanded_rows.append(var)\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS']\n",
    "tocfl_df = pd.DataFrame(rows)[cols + ['Variants']]\n",
    "tocfl_df.to_csv('tocfl.csv', index=False)\n",
    "print('tocfl.csv: %d entries' % len(tocfl_df))\n",
    "assert list(tocfl_df.index) == list(sorted(tocfl_df.index))\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)[cols]\n",
    "expanded_df.to_csv('tocfl-expanded.csv', index=False)\n",
    "assert list(expanded_df.index) == list(sorted(expanded_df.index))\n",
    "print('tocfl-expanded.csv: %d entries' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4874158f-fe2b-4f00-9636-f4ce0f170d5b",
   "metadata": {},
   "source": [
    "## CCCC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ad107-47e8-4482-a64d-cd74bec1ed95",
   "metadata": {},
   "source": [
    "Parse CCCC (Children's Chinese Competency Certification) wordlist, including definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0118063-3a32-4504-9882-d8e08bb0ee9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sheet 1: 萌芽級            \tL1\t474 rows\n",
      "Sheet 2: 成長級            \tL2\t387 rows\n",
      "Sheet 3: 茁壯級            \tL3\t366 rows\n",
      "Total: 1197\n"
     ]
    }
   ],
   "source": [
    "url = 'https://tocfl.edu.tw/assets/files/vocabulary/CCCC_Vocabulary_2022.xls'\n",
    "if os.path.exists('downloads/CCCC_Vocabulary_2022.xls'):\n",
    "    url = 'downloads/CCCC_Vocabulary_2022.xls'\n",
    "\n",
    "xls = pd.ExcelFile(url)\n",
    "\n",
    "sheets = {}\n",
    "for i, name in enumerate(xls.sheet_names[:3]):\n",
    "    df = xls.parse(name, dtype='str', skiprows=1).fillna('')\n",
    "    level = {'萌芽級': 'L1', '成長級': 'L2', '茁壯級': 'L3'}[name]\n",
    "    print(f'Sheet {i+1}: {name:<15}\\t{level}\\t{len(df)} rows')\n",
    "    df = df.rename(columns=lambda s: s.strip().split('\\n')[-1])\n",
    "    df = df.rename(columns={\n",
    "        '分類': 'Category',\n",
    "        '細目': 'Subcategory',\n",
    "        '正體字': 'Traditional',\n",
    "        '简体字': 'Simplified',\n",
    "        '漢拼': 'Pinyin',\n",
    "        '詞性': 'POS',\n",
    "        '英文': 'Meaning',\n",
    "    })\n",
    "    df['ID'] = ['%s-%.3d' % (level, i+1) for i in range(len(df))]\n",
    "    for col in df:\n",
    "        df[col] = df[col].fillna('').str.strip()\n",
    "    df = df[df.Traditional != ''].copy()\n",
    "    sheets[level] = df\n",
    "\n",
    "cccc_excel_df = pd.concat(sheets.values())\n",
    "print(f'Total: {len(cccc_excel_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb88bafe-79f1-421c-b6ea-abf84a17ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cccc.tsv: 1197 rows\n",
      "cccc-expanded.csv: 1344 entries\n"
     ]
    }
   ],
   "source": [
    "def fix_pos(s):\n",
    "    for x, y in [('[;；]', '/'), (' ', ''), ('ADV', 'Adv'), ('VS', 'Vs'), ('[Pp]article', 'Ptc'), ('affix', 'Affix')]:\n",
    "        s = re.sub(x, y, s).strip()\n",
    "    return s\n",
    "\n",
    "rows = []\n",
    "expanded_rows = []\n",
    "\n",
    "for row in cccc_excel_df.fillna('').to_dict(orient='records'):\n",
    "    row['Traditional'] = fix_vocabulary(row['Traditional'])\n",
    "    row['Simplified'] = fix_vocabulary(row['Simplified'])\n",
    "    row['Pinyin'] = fix_pinyin(row['Pinyin'], row['Traditional'])\n",
    "    row['Variants'] = variants_to_json(get_variants(row['Traditional'], row['Pinyin'], row['POS']))\n",
    "    row['POS'] = fix_pos(row['POS'])\n",
    "    row['Simplified'] = to_simplified(row['Traditional'])\n",
    "    rows.append(row)\n",
    "\n",
    "    variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "    for variant in variants:\n",
    "        var = dict(row)\n",
    "        var.update(variant)\n",
    "        expanded_rows.append(var)\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning',\n",
    "        'Category', 'Subcategory']\n",
    "\n",
    "df = pd.DataFrame(rows)[cols + ['Variants']]\n",
    "df.to_csv('cccc.csv', index=False)\n",
    "print('cccc.tsv: %d rows' % len(df))\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)[cols]\n",
    "expanded_df.to_csv('cccc-expanded.csv', index=False)\n",
    "assert list(expanded_df.index) == list(sorted(expanded_df.index))\n",
    "print('cccc-expanded.csv: %d entries' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dfd166-f8f1-4edc-9671-5a6b3c55d1e1",
   "metadata": {},
   "source": [
    "## Export as Pleco user dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ded8f20-785f-44dd-8ad7-54305e8902e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users  94671 Nov  7 13:53 cccc-pleco.txt\n",
      "-rw-r--r-- 1 jovyan users 445784 Nov  7 13:53 tocfl-pleco.txt\n"
     ]
    }
   ],
   "source": [
    "EAC1_TAG = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uEC99'  # tag color, #00cc99 green\n",
    "\n",
    "def gen_pleco(input_fn, output_fn):\n",
    "    with open(output_fn, 'w') as fout:\n",
    "        last_header = ''\n",
    "        for row in pd.read_csv(input_fn, dtype='str').fillna('').to_dict(orient='records'):\n",
    "            cefr = {'0': 'pre-A1', '1': 'A1', '2': 'A2', '3': 'B1', '4': 'B2', '5': 'C1+'}[row['ID'][1]]\n",
    "            if 'cccc' in input_fn:\n",
    "                cefr = {'1': 'pre-A1', '2': 'A1', '3': 'A2'}[row['ID'][1]]\n",
    "                header = f\"//CCCC 2022/Level {row['ID'][1]} ({cefr})\"\n",
    "            elif row['ID'].startswith('L0'):\n",
    "                header = f\"//TOCFL 2023/Novice {row['ID'][3]} ({cefr})\"\n",
    "            else:\n",
    "                header = f\"//TOCFL 2023/Level {row['ID'][1]} ({cefr})\"\n",
    "\n",
    "            if header != last_header:\n",
    "                last_header = header\n",
    "                fout.write(header + '\\n')\n",
    "\n",
    "            variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "            for variant in variants:\n",
    "                var = dict(row)\n",
    "                var.update(variant)\n",
    "                defn = ' '.join([\n",
    "                    f\"{row['Traditional']} [{row['Pinyin']}]\\uEAB1\" if row['Variants'] else '',\n",
    "                    f\"({row['POS']})\" if row.get('POS') else '',\n",
    "                    f\"{row['Meaning']}\" if row.get('Meaning') else '',\n",
    "                    f\"{EAC1_TAG}[CCCC{row['ID'][1]}]\\uEAC2\" if 'cccc' in input_fn else\n",
    "                    f\"{EAC1_TAG}[TOCFL{row['ID'][1]}]\\uEAC2\"\n",
    "                ])\n",
    "                defn = re.sub(r'\\s+', ' ', defn).replace('\\uEAB1 ', '\\uEAB1').strip()\n",
    "                key = f\"{var['Simplified']}[{var['Traditional']}]\\t{var['Pinyin']}\"\n",
    "                fout.write(f'{key}\\t{defn}\\n')\n",
    "\n",
    "gen_pleco('tocfl.csv', 'tocfl-pleco.txt')\n",
    "gen_pleco('cccc.csv', 'cccc-pleco.txt')\n",
    "\n",
    "!ls -l *pleco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fc51a9-dcc8-4104-adca-5c088858d7fa",
   "metadata": {},
   "source": [
    "## Readings check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e3d3216-a8c3-465f-9ed5-8609d2409842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cccc-expanded.csv\n",
      "L2-104 ['去年', '去年', 'qùnian'] vs. ['qunián', 'qùnián']\n",
      "L3-203 ['新鮮', '新鲜', 'xīnxian'] vs. ['xīnxiǎn', 'xīnxiān']\n",
      "tocfl-expanded.csv\n",
      "L1-0079 ['部份', '部份', 'bùfen'] vs. ['bùfèn']\n",
      "L1-0284 ['差不多', '差不多', 'chabùduō'] vs. ['cībúduó', 'cībúduō', 'cībuduó', 'cībuduō', 'cībùduó', 'cībùduō', 'cībūduó', 'cībūduō', 'chābúduó', 'chābúduō', 'chābuduó', 'chābuduō', 'chābùduó', 'chābùduō', 'chābūduó', 'chābūduō', 'chāibúduó', 'chāibúduō', 'chāibuduó', 'chāibuduō', 'chāibùduó', 'chāibùduō', 'chāibūduó', 'chāibūduō', 'chàbúduó', 'chàbúduō', 'chàbuduó', 'chàbuduō', 'chàbùduó', 'chàbùduō', 'chàbūduó', 'chàbūduō']\n",
      "L2-0009 ['白天', '白天', 'báitian'] vs. ['báitiān', 'baitiān']\n",
      "L2-0051 ['差', '差', 'cha'] vs. ['cī', 'chā', 'chāi', 'chà']\n",
      "L2-0414 ['新鮮', '新鲜', 'xīnxian'] vs. ['xīnxiǎn', 'xīnxiān']\n",
      "L3-0205 ['多多少少', '多多少少', 'duōduoshǎoshǎo'] vs. ['duóduóshàoshào', 'duóduóshàoshǎo', 'duóduóshàoshao', 'duóduóshǎoshào', 'duóduóshǎoshǎo', 'duóduóshǎoshao', 'duóduóshaoshào', 'duóduóshaoshǎo', 'duóduóshaoshao', 'duóduōshàoshào', 'duóduōshàoshǎo', 'duóduōshàoshao', 'duóduōshǎoshào', 'duóduōshǎoshǎo', 'duóduōshǎoshao', 'duóduōshaoshào', 'duóduōshaoshǎo', 'duóduōshaoshao', 'duōduóshàoshào', 'duōduóshàoshǎo', 'duōduóshàoshao', 'duōduóshǎoshào', 'duōduóshǎoshǎo', 'duōduóshǎoshao', 'duōduóshaoshào', 'duōduóshaoshǎo', 'duōduóshaoshao', 'duōduōshàoshào', 'duōduōshàoshǎo', 'duōduōshàoshao', 'duōduōshǎoshào', 'duōduōshǎoshǎo', 'duōduōshǎoshao', 'duōduōshaoshào', 'duōduōshaoshǎo', 'duōduōshaoshao']\n",
      "L3-0297 ['古蹟', '古迹', 'gǔjī'] vs. ['gǔjì']\n",
      "L3-0791 ['受得了', '受得了', 'shòudeliao'] vs. ['shòudèle', 'shòudèliǎo', 'shòudéle', 'shòudéliǎo', 'shòuděile', 'shòuděiliǎo', 'shòudele', 'shòudeliǎo', 'shoudèle', 'shoudèliǎo', 'shoudéle', 'shoudéliǎo', 'shouděile', 'shouděiliǎo', 'shoudele', 'shoudeliǎo']\n",
      "L3-1049 ['噢', '噢', 'yǔ'] vs. ['ō']\n",
      "L5-1523 ['奇蹟', '奇迹', 'qíjī'] vs. ['qíjì', 'jījì']\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('../cedict/syllables.csv'):\n",
    "    readings_mp = {} #{'一': set(['yì','yí'])}\n",
    "    syll_df = pd.read_csv('../cedict/syllables.csv', dtype='str').fillna('')\n",
    "    for row in syll_df.itertuples():\n",
    "        readings_mp.setdefault(row.Traditional, set()).add(row.Pinyin.lower())\n",
    "    readings_mp = {x: set([y.strip().lower() for y in readings_mp[x] if y.strip()]) for x in readings_mp}\n",
    "\n",
    "    def gen_readings(trad):\n",
    "        if trad == '':\n",
    "            yield ''\n",
    "        elif trad[0] not in readings_mp or ord(trad[0]) < 0x3E00:\n",
    "            yield from gen_readings(trad[1:])\n",
    "        else:\n",
    "            for x in readings_mp[trad[0]]:\n",
    "                for y in gen_readings(trad[1:]):\n",
    "                    yield x.lower() + (\"'\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "\n",
    "    for filename in ['cccc-expanded.csv', 'tocfl-expanded.csv']:\n",
    "        print(filename)\n",
    "        for row in pd.read_csv(filename, dtype='str').fillna('').itertuples():\n",
    "            trad, pinyin = row.Traditional,row.Pinyin\n",
    "            readings = list(gen_readings(trad))\n",
    "            if pinyin.lower() not in readings:\n",
    "                print(row.ID, list(row._asdict().values())[2:5], 'vs.', readings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b0c648-774d-4384-aa99-806e62ab3632",
   "metadata": {},
   "source": [
    "## Join with CC-CEDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "944e72b6-c656-4426-81b4-6cf6eb69b1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "UNTONE_MP = {\n",
    "    'a': 'a', 'ā': 'a', 'á': 'a', 'ǎ': 'a', 'à': 'a',\n",
    "    'e': 'e', 'ē': 'e', 'é': 'e', 'ě': 'e', 'è': 'e',\n",
    "    'o': 'o', 'ō': 'o', 'ó': 'o', 'ǒ': 'o', 'ò': 'o',\n",
    "    'i': 'i', 'ī': 'i', 'í': 'i', 'ǐ': 'i', 'ì': 'i',\n",
    "    'u': 'u', 'ū': 'u', 'ú': 'u', 'ǔ': 'u', 'ù': 'u',\n",
    "    'ü': 'ü', 'ǖ': 'ü', 'ǘ': 'ü', 'ǚ': 'ü', 'ǜ': 'ü'\n",
    "}\n",
    "\n",
    "# Check if pinyin from the list (py1) matches cedict's (py2)\n",
    "def pinyin_matches(py1, py2, hz='', untone=False):\n",
    "    py1 = py1.lower()\n",
    "    py2 = py2.lower()\n",
    "    i, j = 0, 0\n",
    "    while i < len(py1) or j < len(py2):\n",
    "        a = ''\n",
    "        if i < len(py1):\n",
    "            a = py1[i]\n",
    "            if a in \"-',/() \":\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "        b = ''\n",
    "        if j < len(py2):\n",
    "            b = py2[j]\n",
    "            if b in \"-',/() \":\n",
    "                j += 1\n",
    "                continue\n",
    "\n",
    "        match = (a == b)\n",
    "        match |= untone and (UNTONE_MP.get(a, a) == b or a == UNTONE_MP.get(b, b))\n",
    "        if match:\n",
    "            i += 1\n",
    "            j += 1\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    return i == len(py1) and j == len(py2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2427dd38-28d8-4202-90c6-e0c8397fbae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ambigous simplified: {'ID': 'L0-2234', 'Traditional': '著', 'Simplified': '着', 'Pinyin': 'zhe', 'POS': 'Ptc', 'Variants': ''} ce {'着', '著'} cc 着\n",
      "Ambigous simplified: {'ID': 'L2-0171', 'Traditional': '乾', 'Simplified': '干', 'Pinyin': 'gān', 'POS': 'Vp', 'Variants': ''} ce {'干', '乾'} cc 干\n",
      "No entry for {'ID': 'L2-0178', 'Traditional': '汙染', 'Simplified': '污染', 'Pinyin': 'wūrǎn', 'POS': 'V', 'Variants': ''}\n",
      "No entry for {'ID': 'L2-0292', 'Traditional': '月台', 'Simplified': '月台', 'Pinyin': 'yuètái', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0326', 'Traditional': '還要', 'Simplified': '还要', 'Pinyin': 'háiyào', 'POS': 'Adv', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0335', 'Traditional': '好了', 'Simplified': '好了', 'Pinyin': 'hǎole', 'POS': 'Ptc', 'Variants': ''}\n",
      "No entry for {'ID': 'L3-0809', 'Traditional': '說起來', 'Simplified': '说起来', 'Pinyin': 'shuōqǐlái', 'POS': 'Adv', 'Variants': ''}\n",
      "Ambigous simplified: {'ID': 'L3-1144', 'Traditional': '著', 'Simplified': '着', 'Pinyin': 'zhuó', 'POS': 'V', 'Variants': ''} ce {'着', '著'} cc 着\n",
      "No entry for {'ID': 'L3-1157', 'Traditional': '走走', 'Simplified': '走走', 'Pinyin': 'zǒuzǒu', 'POS': 'Vi', 'Variants': ''}\n",
      "No entry for {'ID': 'L4-1238', 'Traditional': '鈕扣', 'Simplified': '钮扣', 'Pinyin': 'niǔkòu', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L4-1646', 'Traditional': '算帳', 'Simplified': '算帐', 'Pinyin': 'suànzhàng', 'POS': 'V-sep', 'Variants': ''}\n",
      "Simplified diff: {'ID': 'L4-1661', 'Traditional': '牠', 'Simplified': '它', 'Pinyin': 'tā', 'POS': 'N', 'Variants': ''} ce {'牠'} cc 牠\n",
      "Ambigous simplified: {'ID': 'L4-1838', 'Traditional': '閒', 'Simplified': '闲', 'Pinyin': 'xián', 'POS': 'Vs', 'Variants': ''} ce {'闲', '閒'} cc 闲\n",
      "No entry for {'ID': 'L4-1941', 'Traditional': '吸菸', 'Simplified': '吸烟', 'Pinyin': 'xīyān', 'POS': 'V-sep', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-0954', 'Traditional': '艱鉅', 'Simplified': '艰巨', 'Pinyin': 'jiānjù', 'POS': 'Vs', 'Variants': ''}\n",
      "Simplified diff: {'ID': 'L5-1028', 'Traditional': '藉口', 'Simplified': '借口', 'Pinyin': 'jièkǒu', 'POS': 'N', 'Variants': ''} ce {'藉口'} cc 借口\n",
      "No entry for {'ID': 'L5-1523', 'Traditional': '奇蹟', 'Simplified': '奇迹', 'Pinyin': 'qíjī', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-1935', 'Traditional': '台階', 'Simplified': '台阶', 'Pinyin': 'táijiē', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-2106', 'Traditional': '汙染', 'Simplified': '污染', 'Pinyin': 'wūrǎn', 'POS': 'N', 'Variants': ''}\n",
      "No entry for {'ID': 'L5-2155', 'Traditional': '鮮艶/鮮豔', 'Simplified': '鲜艳', 'Pinyin': 'xiānyàn', 'POS': 'Vs', 'Variants': '[{\"Traditional\": \"鮮艶\", \"Simplified\": \"鲜艳\", \"Pinyin\": \"xiānyàn\"}, {\"Traditional\": \"鮮豔\", \"Simplified\": \"鲜艳\", \"Pinyin\": \"xiānyàn\"}]'}\n",
      "No entry for {'ID': 'L5-2373', 'Traditional': '意識到', 'Simplified': '意识到', 'Pinyin': 'yìshìdào', 'POS': 'Vpt', 'Variants': ''}\n",
      "Ambigous simplified: {'ID': 'L5-2445', 'Traditional': '於', 'Simplified': '于', 'Pinyin': 'yú', 'POS': 'Prep', 'Variants': ''} ce {'于', '於'} cc 于\n",
      "Ambigous simplified: {'ID': 'L5-2447', 'Traditional': '餘', 'Simplified': '余', 'Pinyin': 'yú', 'POS': 'N/Vst', 'Variants': ''} ce {'余', '馀'} cc 余\n",
      "No entry for {'ID': 'L5-2692', 'Traditional': '轉帳', 'Simplified': '转帐', 'Pinyin': 'zhuǎnzhàng', 'POS': 'V-sep', 'Variants': ''}\n"
     ]
    }
   ],
   "source": [
    "tocfl_df = pd.read_csv('tocfl.csv', dtype='str').fillna('')\n",
    "cedict_df = pd.read_csv('../cedict/cedict.csv', dtype='str').fillna('')\n",
    "cedict_idx_mp = cedict_df.assign(idx=cedict_df.index).groupby('Traditional').idx.apply(list)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for row in tocfl_df.to_dict(orient='records'):\n",
    "    pinyin_set = set([row['Pinyin']])\n",
    "    matches = cedict_idx_mp.get(row['Traditional'], [])\n",
    "    if len(matches) == 0 and row['Variants']:\n",
    "        variants = json.loads(row['Variants']) if row['Variants'] else [{}]\n",
    "        for variant in variants:\n",
    "            matches.extend(cedict_idx_mp.get(variant['Traditional'], []))\n",
    "            pinyin_set.add(variant['Pinyin'])\n",
    "\n",
    "    matches = list(sorted(set(matches)))\n",
    "\n",
    "    if len(matches) == 0:\n",
    "        print('No entry for %s' % row)\n",
    "    else:\n",
    "        # Prioritize pronunciation matches, downpriorize names and variants\n",
    "        # TODO: match based on taiwanese pronunciation\n",
    "        if len(matches) > 1:\n",
    "            matches.sort(key=lambda i: (\n",
    "                -int(any(pinyin_matches(py, cedict_df.Pinyin[i], untone=False) for py in pinyin_set))\n",
    "                -int(any(pinyin_matches(py, cedict_df.Pinyin[i], untone=True) for py in pinyin_set))\n",
    "                +10*int(re.match('^variant', cedict_df.Definitions[i]) is not None)\n",
    "                +100*int(cedict_df.Pinyin[i][0].isupper())\n",
    "            ))\n",
    "\n",
    "        ce_simp = set([cedict_df.Simplified[i] for i in matches])\n",
    "        cc_simp = opencc_tw2s.convert(row['Traditional'])\n",
    "        if not row['Variants'] and ce_simp:\n",
    "            if row['Simplified'] not in ce_simp:\n",
    "                print('Simplified diff:', row, 'ce', ce_simp, 'cc', cc_simp)\n",
    "            if len(ce_simp) > 1:\n",
    "                print('Ambigous simplified:', row, 'ce', ce_simp, 'cc', cc_simp)\n",
    "\n",
    "        defs = []\n",
    "        for i in matches:\n",
    "            py1 = list(pinyin_set)[0] if len(pinyin_set) == 1 else ''\n",
    "            defn = cedict_df.Definitions[i]\n",
    "            defn = re.sub(r'/CL:個\\|个\\[ge4\\](|/.*)$', r'\\1', defn)  # uninformative\n",
    "            if row['Variants']:\n",
    "                defn = '%s [%s] %s' % (cedict_df.Traditional[i], cedict_df.Pinyin[i], defn)\n",
    "            elif not pinyin_matches(py1, cedict_df.Pinyin[i], untone=False):\n",
    "                defn = '[%s] %s' % (cedict_df.Pinyin[i], defn)\n",
    "            defs.append(defn)\n",
    "\n",
    "        row['Meaning'] = '<br> '.join(defs)\n",
    "\n",
    "    rows.append(row)\n",
    "\n",
    "merged_df = pd.DataFrame(rows)\n",
    "merged_df.to_csv('tocfl-cedict.csv', index=False)\n",
    "\n",
    "# diffs mostly due to variants chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3edf60-e80b-4c1e-bd10-9efaa9ff6e1e",
   "metadata": {},
   "source": [
    "## Generate Anki deck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b63a0-7838-483f-848f-d324ea27019e",
   "metadata": {},
   "source": [
    "*Taiwan TOCFL 2023 wordlist with audio (Traditional)*\n",
    "\n",
    "Complete wordlist of TOCFL (Test of Chinese as a Foreign Language), a taiwanese equivalent of HSK.\n",
    "\n",
    "Parsed from official excel sheets from [TOCFL](https://tocfl.edu.tw/) website. This is a new 2022/2023 version ([8000zhuyin_202307.zip](https://tocfl.edu.tw/assets/files/vocabulary/8000zhuyin_202307.zip)) of the list with 7517 entries (previous 2018 list had 7945 entries.)\n",
    "\n",
    "Columns:\n",
    "  * `ID`: term's level + index (row number in original excel file which has one sheet per level):\n",
    "    * `L0-1nnn` = Novice 1 (準備級一級), `L0-2nnn` = Novice 2 (準備級二級), both pre-A1, `L1-nnnn`..`L5-nnnn` = Level 1..5 (入門級/基礎級/進階級/高階級/流利級) = CEFR A1/A2/B1/B2/C1+.\n",
    "    * Levels are also added as tags.\n",
    "  * `Traditional`: term in traditional characters per TOCFL.\n",
    "  * `Simplified`: term converted to simplified characters.\n",
    "  * `Pinyin`: pinyin with diacritics, slightly cleaned up from TOCFL sheets, e.g. missing apostrophes added and a few clear errors corrected. Tone changes are not indicated.\n",
    "  * `POS`: part of speech, `/`-separated. See [description](https://tocfl.edu.tw/assets/files/vocabulary/8000_description_202204.pdf) on TOCFL website for the meaning of abbreviations (202204 list is essentially same)\n",
    "  * `Meaning`: definitions from [CC-CEDICT](https://www.mdbg.net/chinese/dictionary?page=cedict) for convenience. Note it mainly lists mainland pronunciations which may differ from taiwanese in some cases. [CC-BY-SA 4.0](https://creativecommons.org/licenses/by-sa/4.0/) licensed.\n",
    "  * `Audio`: good quality neural TTS audio with a taiwanese mandarin voice.\n",
    "  * `Variants`: for entries where TOCFL gives multiple variants of a term, an expanded disambiguated list as a JSON list of objects with alternatives column values. If using this deck for an automatic analysis (such as merging with other sources or your anki decks), you might find this field useful as the original source is inconsistent in formatting variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cc9e4dd-fb8a-4bac-9b26-a4c77024362d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 153082035 Nov  7 13:53 tocfl.apkg\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('tocfl-cedict.csv', dtype='str').fillna('')\n",
    "df['Audio'] = ''\n",
    "\n",
    "!mkdir -p data/media\n",
    "!cp -f ../downloads/fonts/MoeStandardKai.ttf data/media/_MoeStandardKai.ttf\n",
    "if os.path.exists('../Anki2/hypertts.tsv'):\n",
    "    tts_mp = pd.read_csv('../Anki2/hypertts.tsv', sep='\\t').set_index('Text').Hash.to_dict()\n",
    "    for row in df.itertuples():\n",
    "        text = json.loads(row.Variants)[0]['Traditional'] if row.Variants else row.Traditional\n",
    "        dst = 'data/media/tocfl-tts-%s.mp3' % text\n",
    "        if not os.path.exists(dst) and text in tts_mp:\n",
    "            shutil.copy('../Anki2/tts/collection.media/hypertts-%s.mp3' % tts_mp[text], 'data/media/tocfl-tts-%s.mp3' % text)\n",
    "        df.loc[row.Index, 'Audio'] = '[sound:tocfl-tts-%s.mp3]' % text\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Variants', 'Audio']\n",
    "\n",
    "model = genanki.Model(\n",
    "    1698016000,\n",
    "    'TOCFL',\n",
    "    fields=[{'name': c} for c in cols],\n",
    "    templates=[{\n",
    "        'name': 'TOCFL',\n",
    "        'qfmt': open('../dangdai/dangdai-qfmt.html').read(),\n",
    "        'afmt': open('../dangdai/dangdai-afmt.html').read().replace(\n",
    "            'if (pinyinEl && hanziEl)',\n",
    "            'if (pinyinEl && hanziEl {{#Variants}}&& false{{/Variants}})'),\n",
    "    }],\n",
    "    css=open('../dangdai/dangdai.css').read(),\n",
    ")\n",
    "\n",
    "deck = genanki.Deck(1698016001, name='tocfl')\n",
    "\n",
    "for row in df.reset_index().to_dict(orient='records'):\n",
    "    note = genanki.Note(\n",
    "        model=model,\n",
    "        fields=[row[c] for c in cols],\n",
    "        guid=genanki.guid_for('tocfl', row['ID']),\n",
    "        tags=[row['ID'][:2]],\n",
    "    )\n",
    "    deck.add_note(note)\n",
    "\n",
    "!rm -f tocfl.apkg\n",
    "genanki.Package(deck, media_files=glob.glob('data/media/*')).write_to_file('tocfl.apkg')\n",
    "!ls -l tocfl.apkg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
