{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a115e903-1c32-4a7a-bc1d-195ad1dcc3ac",
   "metadata": {},
   "source": [
    "# Modern Chinese (時代華語)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1454ae-2464-4b44-b7cc-f2166c3d3e81",
   "metadata": {},
   "source": [
    "Parser for book's vocabulary from slides from its supplementary website: https://sites.google.com/clc.tku.edu.tw/modernchinese-official/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7efa796-7012-4bd4-afcd-0573386a3fb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q python-pptx genanki opencc\n",
    "\n",
    "import glob, os, re, json\n",
    "import pandas as pd\n",
    "import genanki\n",
    "import opencc\n",
    "from pptx import Presentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69769749-a668-4d02-a6d4-f18e1ee267c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify to point to a copy of the slides if you need to rerun slides parser.\n",
    "PPTX_PATHS = {}\n",
    "for d in sorted(glob.glob('downloads/B?L??')):\n",
    "    pp = glob.glob(d + '/*.pptx')\n",
    "    pp = [s for s in pp if not re.match('.*(_0617.pptx|短文速讀|學習單1|學習單2附件).*', s)]\n",
    "    assert len(pp) == 1\n",
    "    PPTX_PATHS[os.path.basename(d)] = pp[0]\n",
    "assert len(PPTX_PATHS) == 16*4+1\n",
    "\n",
    "# Download ex.\n",
    "# url=\"https://sites.google.com/clc.tku.edu.tw/modernchinese-official/%E7%AC%AC%E4%B8%80%E5%86%8A/b1-l1\"\n",
    "# for driveid in $(curl \"$url\" |\n",
    "#                  egrep -o '<iframe[^>]*drive.google.com[^>]*preview[^>]*>' |\n",
    "#                  sed -Ee 's|.*https://drive.google.com/file/d/([^/]+)/preview.*|\\1|'); do\n",
    "#   rclone backend copyid drive: \"$driveid\" ./\n",
    "# done\n",
    "#\n",
    "# rclone via docker/podman:\n",
    "# podman run -v ~/.config/rclone:/config/rclone:rw -v \"$PWD:/pwd:rw\" docker.io/rclone/rclone backend copyid drive: \"$driveid\" /pwd/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f20f27c5-603d-4ddd-9249-dc6d5392ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "opencc_tw2s = opencc.OpenCC('tw2s')\n",
    "\n",
    "# Character levels from Table of General Standard Chinese Characters for verification.\n",
    "tgh_level = pd.read_csv('../chars/tgh.csv').set_index('char').level.to_dict()\n",
    "\n",
    "# Convert to simplified characters + verify\n",
    "def to_simplified(trad):\n",
    "    simp = opencc_tw2s.convert(trad)\n",
    "    for x, y in ('擡抬', '砲炮', '妳你'):\n",
    "        simp = simp.replace(x, y)\n",
    "    if '/' in simp and len(set(simp.split('/'))) == 1:\n",
    "        simp = simp.split('/')[0]\n",
    "    for c in simp:\n",
    "        assert tgh_level.get(c, 9) <= 2 or c in '/（），？…101 3C KTV BBC OK 蚵', (trad, simp, c, tgh_level.get(c))\n",
    "    return simp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e91f0d-06a7-4a2c-81bc-02cbf6ff74d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore slides layout\n",
    "\n",
    "if 0:\n",
    "    rows = []\n",
    "    for book in ['B1', 'B2', 'B3', 'B4']:\n",
    "        for lesson, filepath in PPTX_PATHS.items():\n",
    "            if not lesson.startswith(book): continue\n",
    "            prs = Presentation(filepath)\n",
    "            for i, slide in enumerate(prs.slides):\n",
    "                rows.append({'Book': book, 'Lesson': lesson, 'Page': i+1, 'Layout': slide.slide_layout.name})\n",
    "    df = pd.DataFrame(rows)\n",
    "    print(df.groupby('Book').Layout.value_counts())\n",
    "\n",
    "def explore(lesson, page=None):\n",
    "    book = lesson[:2]\n",
    "    filepath = PPTX_PATHS[lesson]\n",
    "    prs = Presentation(filepath)\n",
    "    res = []\n",
    "    for slide_i, slide in enumerate(prs.slides):\n",
    "        if page and slide_i+1 != page: continue\n",
    "        paragraphs = []\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame: continue\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                text = ''.join(run.text for run in paragraph.runs).strip()\n",
    "                if not text: continue\n",
    "                pidx = shape.placeholder_format.idx if shape.is_placeholder else None\n",
    "                #if (book, slide.slide_layout.name, pidx) in placeholders_mapping: continue\n",
    "                paragraphs.append((text, pidx))\n",
    "        if len(paragraphs) == 0: continue\n",
    "        res.append({'page': slide_i+1, 'layout': slide.slide_layout.name, 'paragraphs': paragraphs})\n",
    "\n",
    "    return res\n",
    "\n",
    "#explore('B2L07', 82)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ec13a5-ed7b-4d77-91a6-51362cc128dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from slides\n",
    "\n",
    "# book/lesson, layout, placeholder idx -> field\n",
    "PLACEHOLDER_MAP = {\n",
    "    ('B1L00', ' 生詞', 10): 'Traditional',\n",
    "    ('B1L00', ' 生詞', 11): 'Pinyin',\n",
    "    ('B1L00', ' 生詞', 12): 'POS',\n",
    "    ('B1L00', ' 生詞', 13): 'Meaning',\n",
    "    ('B1', ' 生詞', 1): 'Traditional',\n",
    "    ('B1', ' 生詞', 2): 'Pinyin',\n",
    "    ('B1', ' 生詞', 3): 'Example',\n",
    "    ('B1', ' 生詞', 4): 'ExamplePinyin',\n",
    "    ('B1', ' 生詞', 5): 'POS',\n",
    "    ('B1', ' 生詞', 6): 'Meaning',\n",
    "    ('B1', ' 生詞', 10): 'Traditional',\n",
    "    ('B1', ' 生詞', 11): 'Pinyin',\n",
    "    ('B1', ' 生詞', 12): 'Example',\n",
    "    ('B1', ' 生詞', 13): 'ExamplePinyin',\n",
    "    ('B1', ' 生詞', 14): 'POS',\n",
    "    ('B1', ' 生詞', 15): 'Meaning',\n",
    "    ('B1', ' 生詞', None): 'Example2',\n",
    "    ('B2', '自訂版面配置', 1): 'Traditional',\n",
    "    ('B2', '自訂版面配置', 2): 'Meaning',\n",
    "    ('B2', '自訂版面配置', 3): 'Pinyin',\n",
    "    ('B2', '生詞', 1): 'Example',\n",
    "    ('B2', '生詞', 2): 'Traditional',\n",
    "    ('B2', '生詞', 3): 'Meaning',\n",
    "    ('B2', '生詞', 4): 'Pinyin',\n",
    "    ('B2', '生詞', 10): 'Example',\n",
    "    ('B2', '生詞', 13): 'Traditional',\n",
    "    ('B2', '生詞', 15): 'Meaning',\n",
    "    ('B2', '生詞', 17): 'Pinyin',\n",
    "    ('B2', '生詞_1', 10): 'Example',\n",
    "    ('B2', '生詞_1', 13): 'Traditional',\n",
    "    ('B2', '生詞_1', 15): 'Meaning',\n",
    "    ('B2', '生詞_1', 17): 'Pinyin',\n",
    "    ('B2', '1_生詞', 13): 'Traditional',\n",
    "    ('B2', '1_生詞', 15): 'Meaning',\n",
    "    ('B2', '1_生詞', 17): 'Pinyin',\n",
    "    ('B2', '2_生詞_1', 13): 'Traditional',\n",
    "    ('B2', '2_生詞_1', 15): 'Meaning',\n",
    "    ('B2', '2_生詞_1', 17): 'Pinyin',\n",
    "    ('B2', '1_生詞_1', 13): 'Traditional',\n",
    "    ('B2', '1_生詞_1', 15): 'Meaning',\n",
    "    ('B2', '1_生詞_1', 17): 'Pinyin',\n",
    "    ('B3', '標題及內容', None): 'Freetext',\n",
    "    ('B3', 'OBJECT', None): 'Freetext',\n",
    "    ('B3', '標題投影片', None): 'TitleSlide',\n",
    "    ('B3', '4_標題投影片', None): 'TitleSlide',\n",
    "    ('B3', 'TITLE', None): 'TitleSlide',\n",
    "    ('B4', '生詞_有例句', 10): 'Example',\n",
    "    ('B4', '生詞_有例句', 13): 'Traditional',\n",
    "    ('B4', '生詞_有例句', 15): 'Meaning',\n",
    "    ('B4', '生詞_有例句', 17): 'Pinyin',\n",
    "    ('B4', '生詞_無例句', 13): 'Traditional',\n",
    "    ('B4', '生詞_無例句', 15): 'Meaning',\n",
    "    ('B4', '生詞_無例句', 17): 'Pinyin',    \n",
    "    ('B4', '生詞_無例句', 1): 'Traditional',\n",
    "    ('B4', '生詞_無例句', 2): 'Meaning',\n",
    "    ('B4', '生詞_無例句', 3): 'Pinyin',    \n",
    "    ('B4', '生詞_有例句', 1): 'Example',\n",
    "    ('B4', '生詞_有例句', 2): 'Traditional',\n",
    "    ('B4', '生詞_有例句', 3): 'Meaning',\n",
    "    ('B4', '生詞_有例句', 4): 'Pinyin',    \n",
    "    ('B4', '短語', 1): 'Traditional',\n",
    "    ('B4', '短語', 2): 'Meaning',\n",
    "    ('B4', '短語', 3): 'Pinyin',\n",
    "    ('B4', '短語', 4): 'Example',\n",
    "    ('B4', '短語', 13): 'Traditional',\n",
    "    ('B4', '短語', 15): 'Meaning',\n",
    "    ('B4', '短語', 17): 'Pinyin',\n",
    "}\n",
    "\n",
    "entries = []\n",
    "\n",
    "for lesson, filepath in PPTX_PATHS.items():\n",
    "    book = lesson[:2]\n",
    "    prs = Presentation(filepath)\n",
    "\n",
    "    for slide_i, slide in enumerate(prs.slides):\n",
    "        row = {\n",
    "            'Book': lesson[:2],\n",
    "            'Lesson': lesson,\n",
    "            'Page': slide_i+1,\n",
    "            'Layout': slide.slide_layout.name\n",
    "        }\n",
    "\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame:\n",
    "                continue\n",
    "\n",
    "            for paragraph in shape.text_frame.paragraphs:\n",
    "                text = ''.join(run.text for run in paragraph.runs).strip()\n",
    "                assert '\\n' not in text\n",
    "\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                pidx = shape.placeholder_format.idx if shape.is_placeholder else None\n",
    "\n",
    "                field = PLACEHOLDER_MAP.get((lesson, slide.slide_layout.name, pidx))\n",
    "                if not field:\n",
    "                    field = PLACEHOLDER_MAP.get((book, slide.slide_layout.name, pidx))\n",
    "                if not field and (text.lower() in ('names', 'name', 'phrase', 'phrases')):\n",
    "                    field = 'POS'\n",
    "                if not field:\n",
    "                    continue\n",
    "\n",
    "                if field == 'ExamplePinyin':\n",
    "                    continue\n",
    "\n",
    "                if field in row:\n",
    "                    assert '<' not in text\n",
    "                    row[field] += '<br>' + text\n",
    "                else:\n",
    "                    row[field] = text\n",
    "\n",
    "        if len(row) == 5 and 'Freetext' in row and len(row['Freetext']) < 10:\n",
    "            continue\n",
    "\n",
    "        if len(row) > 4:\n",
    "            entries.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "608ada4e-e252-4080-978d-88c399679b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detects spans of consecutive pages with defs and assign IDs.\n",
    "\n",
    "span = None\n",
    "spans = {}\n",
    "\n",
    "for i, row in enumerate(entries):\n",
    "    if span:\n",
    "        if row['Lesson'] == span[0] and row['Page'] == entries[i-1]['Page']+1:\n",
    "            span[2] = row['Page']\n",
    "            span[3].append(row)\n",
    "            continue\n",
    "    if span:\n",
    "        spans.setdefault(span[0], []).append(span)\n",
    "    span = [row['Lesson'], row['Page'], row['Page'], [row]]\n",
    "spans.setdefault(span[0], []).append(span)\n",
    "\n",
    "for lesson in spans:\n",
    "    #print(lesson, [s[1:3] for s in spans[lesson]])\n",
    "    assert len(spans[lesson]) <= 3\n",
    "    for span_i, span in enumerate(spans[lesson]):\n",
    "        title = None\n",
    "        k = 0\n",
    "        prev_row = None\n",
    "        for row in span[3]:\n",
    "            if 'TitleSlide' in row:\n",
    "                title = row['TitleSlide']\n",
    "                continue\n",
    "            if title:\n",
    "                row['Title'] = title\n",
    "            row['Span'] = span_i + 1\n",
    "\n",
    "            if lesson + row.get('Traditional', '') in ['B1L12事(情)', 'B1L15代表', 'B1L13壞', 'B1L12工作'] and prev_row and \\\n",
    "                prev_row['Traditional'] == row['Traditional']:\n",
    "                prev_row['Example'] = prev_row.get('Example', '') + '<br>' + row.get('Example', '')\n",
    "                row['Drop'] = 1\n",
    "                continue\n",
    "            prev_row = row\n",
    "\n",
    "            k += 1\n",
    "            row['ID'] = '%s-%d-%02d' % (row['Lesson'], row['Span'], k)\n",
    "\n",
    "entries = [row for row in entries if 'Drop' not in row]\n",
    "entries = [row for row in entries if 'TitleSlide' not in row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "152e239c-d0df-452d-8fe0-f8cfdc8845cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse free text on B3 slides which don't use placeholders\n",
    "\n",
    "def cn_score(text):\n",
    "    text = re.sub('([\\t0-9 （）(  ) …、/‧-]|-$)', '', text)\n",
    "    n = len(text)\n",
    "    if n == 0: return 0\n",
    "    k = sum(ord(c) >= 0x4E00 for c in text)\n",
    "    if k == 0: return 0\n",
    "    if k == n: return 1\n",
    "    return k / n\n",
    "\n",
    "ACC_POS = set(['Adv', 'Adv/ Vs-attr', 'Adv/Vs', 'Conj', 'Conj/Prep', 'Det', 'M', 'M/N', 'N',\n",
    "               'N/V', 'N/Vi', 'N/Vs', 'Phrases', 'Prep', 'Ptc', 'V', 'V-sep', 'V/N', 'V/Vi',\n",
    "               'V/Vs', 'Vaux', 'Vi', 'Vi/N', 'Vi/Vs', 'Vp', 'Vp-sep', 'Vp/N', 'Vpt', 'Vpt/N',\n",
    "               'Vs', 'Vs-attr', 'Vs-attr/Adv', 'Vs-attr/Vi', 'Vs-pred', 'Vs-sep', 'Vs/Adv',\n",
    "               'Vs/N', 'Vs/Vst', 'Vst', 'adv'])\n",
    "\n",
    "ss = []\n",
    "\n",
    "for row in entries:\n",
    "    if row['Book'] != 'B3': continue\n",
    "    text = row['Freetext'].split('<br>')\n",
    "\n",
    "    sc = [cn_score(s) for s in text]\n",
    "    if sc[:3] == [1, 1, 0]:\n",
    "        text = [text[0] + text[1]] + text[2:]\n",
    "    elif sc[:2] == [0, 0]:\n",
    "        hanzi = {'guāi': '乖', 'dǎo': '倒', 'diàochá': '調查', 'shìyě': '視野', 'shǔ': '數',\n",
    "                 'xiōngdì': '兄弟', 'wúguān': '無關', 'jiàoxué': '教學', 'bìng': '病',\n",
    "                 'wǎngyǒu': '網友', 'kòng': '空', 'kōng': '空', 'ài': '愛', 'tiānzhēn': '天真',\n",
    "                 'yǐlái': '老家', 'réngrán': '仍然'}[text[0]]\n",
    "        i = text.index(hanzi)\n",
    "        text = [hanzi] + text[:i] + text[(i+1):]\n",
    "    sc = [cn_score(s) for s in text]\n",
    "    assert sc[:3] == [1, 0, 0]\n",
    "\n",
    "    if len(text) >= 4 and text[2] not in ACC_POS and text[3] in ACC_POS:\n",
    "        text = text[:1] + [text[1] + ' ' + text[2]] + text[3:]\n",
    "    if text[2] not in ACC_POS and sum(s in ACC_POS for s in text) == 1:\n",
    "        i = [int(s in ACC_POS) for s in text].index(1)\n",
    "        assert i != 0\n",
    "        if i == 1 and any(c in 'āáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ' for c in text[-1]):\n",
    "            text = [text[0], text[-1]] + text[1:-1]\n",
    "        elif i == 1 and any(c in 'āáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ' for c in text[3]):\n",
    "            text = [text[0], text[3], text[1], text[2]] + text[4:]\n",
    "        else:\n",
    "            assert i != 1\n",
    "            text = text[:2] + [text[i]] + text[2:i] + text[(i+1):]\n",
    "\n",
    "    if text[2] not in ACC_POS:\n",
    "        for x,y in [('Wángmǔ', 'Niángniang'), ('shèqún', 'wǎngzhàn')]:\n",
    "            if x in text:\n",
    "                i = text.index(x)\n",
    "                assert i+1 == text.index(y)\n",
    "                text = text[:i] + [text[i] + ' ' + text[i+1]] + text[(i+2):]\n",
    "\n",
    "    if text[2] not in ACC_POS:\n",
    "        text = text[:2] + [''] + text[2:]\n",
    "\n",
    "    sc = [0,0,0] + [int(cn_score(s) > 0.5) for s in text[3:]]\n",
    "    if 1 in sc:\n",
    "        i = sc.index(1)\n",
    "        if i == 3:\n",
    "            assert sc[-1] == 0\n",
    "            row['Example'] = ' '.join(text[i:-1])\n",
    "            text = text[:i] + [text[-1]]\n",
    "        else:\n",
    "            row['Example'] = ' '.join(text[i:])\n",
    "            text = text[:i]\n",
    "\n",
    "    row['Traditional'] = text[0]\n",
    "    row['Pinyin'] = text[1]\n",
    "    row['POS'] = text[2]\n",
    "    row['Meaning'] = ' '.join(text[3:]).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "811f5401-d285-4a1a-99d2-2155c6a90a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and normalize POS\n",
    "\n",
    "POS_MAP = {\n",
    "  '(Prep )': 'Prep',\n",
    "  '(N, V)': 'N/V',\n",
    "  '(Vi, N)': 'Vi/N',\n",
    "  '(N, Vs)': 'N/Vs',\n",
    "  '(N, M)': 'N/M',\n",
    "  '(N, V)': 'N/V',\n",
    "  '(N, V)': 'N/V',\n",
    "  '(Adv)': 'Adv',\n",
    "  '(Adv.)': 'Adv',\n",
    "  '(Adv.,Vs)': 'Adv/Vs',\n",
    "  '(Adv./Vs)': 'Adv/Vs',\n",
    "  '(Conj)': 'Conj',\n",
    "  '(Conj.)': 'Conj',\n",
    "  '(Det)': 'Det',\n",
    "  '(M)': 'M',\n",
    "  '(M,N)': 'M/N',\n",
    "  '(M/N)': 'M/N',\n",
    "  '(M/V)': 'M/V',\n",
    "  '(N)': 'N',\n",
    "  '(N,V)': 'N/V',\n",
    "  '(N,Vs-attr)': 'N/Vs-attr',\n",
    "  '(N/M)': 'N/M',\n",
    "  '(N/V)': 'N/V',\n",
    "  '(N/V-sep)': 'N/V-sep',\n",
    "  '(N/Vi)': 'N/Vi',\n",
    "  '(N/Vp-sep)': 'N/Vp-sep',\n",
    "  '(N/Vs)': 'N/Vs',\n",
    "  '(N/Vst)': 'N/Vst',\n",
    "  '(N/Vst/V)': 'N/Vst/V',\n",
    "  '(Phrase)': 'Phrase',\n",
    "  '(Prep)': 'Prep',\n",
    "  '(Prep/V/Vst)': 'Prep/V/Vst',\n",
    "  '(Ptc)': 'Ptc',\n",
    "  '(V)': 'V',\n",
    "  '(V-sep)': 'V-sep',\n",
    "  '(V-sep/N)': 'V-sep/N',\n",
    "  '(V/Adv.)': 'V/Adv',\n",
    "  '(V/N)': 'V/N',\n",
    "  '(V/Vs)': 'V/Vs',\n",
    "  '(Vaux)': 'Vaux',\n",
    "  '(Vaux/V)': 'Vaux/V',\n",
    "  '(Vi)': 'Vi',\n",
    "  '(Vi,N)': 'Vi/N',\n",
    "  '(Vi/Adv)': 'Vi/Adv',\n",
    "  '(Vi/N)': 'Vi/N',\n",
    "  '(Vi/V-sep)': 'Vi/V-sep',\n",
    "  '(Vp)': 'Vp',\n",
    "  '(Vp-sep)': 'Vp-sep',\n",
    "  '(Vp/Vpt)': 'Vp/Vpt',\n",
    "  '(Vp/Vs)': 'Vp/Vs',\n",
    "  '(Vpt)': 'Vpt',\n",
    "  '(Vpt/N)': 'Vpt/N',\n",
    "  '(Vpt/Vp)': 'Vpt/Vp',\n",
    "  '(Vs)': 'Vs',\n",
    "  '(Vs-attr / Vi)': 'Vs-attr/Vi',\n",
    "  '(Vs-attr/Vp)': 'Vs-attr/Vp',\n",
    "  '(Vs-attr)': 'Vs-attr',\n",
    "  '(Vs-attr/Adv.)': 'Vs-attr/Adv',\n",
    "  '(Vs-attr/Vp)': 'Vs-attr/Vp',\n",
    "  '(Vs-pred)': 'Vs-pred',\n",
    "  '(Vs-sep)': 'Vs-sep',\n",
    "  '(Vs/V)': 'Vs/V',\n",
    "  '(Vs/Vst)': 'Vs/Vst',\n",
    "  '(Vst)': 'Vst',\n",
    "  '(Vst/N)': 'Vst/N',\n",
    "  '(Vst/Prep)': 'Vst/Prep',\n",
    "  '(Vst/Vs/Adv./Vi)': 'Vst/Vs/Adv/Vi',\n",
    "  '（Det)': 'Det',\n",
    "  '(Vs-attr, N)': 'Vs-attr/N',\n",
    "  '(Vs-attr': 'Vs-attr',\n",
    "  '(Vs-attr,': 'Vs-attr,',\n",
    "}\n",
    "POS_REMAP = {\n",
    "  'name': 'Name',\n",
    "  'Names': 'Name',\n",
    "  'Phrase': 'Ph',\n",
    "  'phrase': 'Ph',\n",
    "  'phrases': 'Ph',\n",
    "  'Phrases': 'Ph',\n",
    "  'VS': 'Vs',\n",
    "  'Vs-attr,': 'Vs-attr',\n",
    "  'adv': 'Adv',\n",
    "  'Cong': 'Conj',\n",
    "  'Adv/ Vs-attr': 'Adv/Vs-attr',\n",
    "  'V-sep/ N': 'V-sep/N',\n",
    "}\n",
    "\n",
    "for row in entries:\n",
    "    text = row.get('Meaning', '').replace('<br>', ' ').strip()\n",
    "    text0 = text\n",
    "    pos = ''\n",
    "    for pref in POS_MAP:\n",
    "        if text.startswith(pref):\n",
    "            pos = POS_MAP[pref]\n",
    "            text = text[len(pref):]\n",
    "    if not pos and 'measure' in text.lower() or 'classifier' in text.lower():\n",
    "        pos = 'M'\n",
    "    if not pos and not row.get('POS') and row.get('Title') == 'Phrase':\n",
    "        pos = 'Phrase'\n",
    "    if not pos and not row.get('POS') and row.get('Title') == 'Names':\n",
    "        pos = 'Name'\n",
    "    if pos:\n",
    "        assert row.get('POS', pos) in (pos, ''), (row, pos)\n",
    "        row['POS'] = pos\n",
    "    pos = row.get('POS', '')\n",
    "    pos = pos.strip()\n",
    "    pos = POS_REMAP.get(pos, pos)\n",
    "    row['POS'] = pos\n",
    "    text = text.strip()\n",
    "    row['Meaning'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e495feb9-5f56-4764-8a47-14a2e1ee61fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup/normalize entries\n",
    "\n",
    "variants_df = pd.read_csv('variants.csv', dtype='str').fillna('').set_index('ID')\n",
    "\n",
    "def variants_to_json(variants):\n",
    "    arr = []\n",
    "    for var in variants.split(' / '):\n",
    "        m = re.match(r'^([^ ()\\[\\]]+) \\[([^()\\[\\]]+)\\]$', var)\n",
    "        assert m, variants\n",
    "        arr.append({\n",
    "            'Traditional': m[1],\n",
    "            'Simplified': to_simplified(m[1]),\n",
    "            'Pinyin': m[2]\n",
    "        })\n",
    "    return json.dumps(arr, ensure_ascii=False)\n",
    "\n",
    "for row in entries:\n",
    "    if not row.get('Meaning') and row['Traditional'] == '保護':\n",
    "        row['POS'], row['Meaning'] = 'V', 'to protect'\n",
    "    if not row.get('Pinyin'):\n",
    "        row['Pinyin'] = {'隻': 'zhī', 'KTV': 'KTV'}[row['Traditional']]\n",
    "    if not row.get('Traditional'):\n",
    "        row['Traditional'] = {'yìnzhāng/túzhāng': '印章/圖章'}[row['Pinyin']]\n",
    "\n",
    "    trad = row['Traditional']\n",
    "    for x, y in (('<br>', ''), (' ', ''), (',', '，'), ('(', '（'), (')', '）'), ('／', '/'), ('∕', '/'), ('。', ''),\n",
    "                 ('髪', '髮'),\n",
    "                ):\n",
    "        trad = trad.replace(x, y).strip()\n",
    "\n",
    "    trad = {\n",
    "        '你/妳好': '你好/妳好',\n",
    "        '計畫/劃': '計畫/計劃',\n",
    "        '部分/份': '部分/部份',\n",
    "        '健康檢查（健檢）': '健康檢查/健檢',\n",
    "        '潮溼，潮濕': '潮溼/潮濕',\n",
    "        '差（一）點兒': '差（一）點',\n",
    "        '白白/白（白）': '白白/白',\n",
    "        # 裏: HK variant - not in TW font, dropping due to rendering problem. These variants are in TOCFL though\n",
    "        '夜裡（裏）': '夜裡',  #'夜裡/夜裏', \n",
    "        '裡頭/裏頭': '裡頭',\n",
    "    }.get(trad, trad)\n",
    "    assert re.match(r'^([\\u4e00-\\u9fff（），/]|KTV|101|卡拉OK|3C產品|……|？$)+$', trad)\n",
    "    row['Traditional'] = trad\n",
    "\n",
    "    pinyin = row['Pinyin']\n",
    "    for x, y in (('<br>', ' '), ('\\t', ' '), (' ', ' '), ('\\u200b', ' '), ('\\ufeff', ' '),\n",
    "                 ('\\u00a0', ' '), ('  ', ' '), (' , ', ', '), ('’', \"'\"), (' / ', '/'),\n",
    "                 ('/ ', '/'), (' /', '/'), (' \\ ', '/'), (' ?', '?'), ('‑', ' '),\n",
    "                 ('-', ' '), ('ă', 'ǎ'), ('ĕ', 'ě'), ('ĭ', 'ǐ'), ('ŏ', 'ǒ'), ('ŭ', 'ǔ'),\n",
    "                 ('ǎ', 'ǎ'), ('a\\u030c', 'ǎ'), ('e\\u030c', 'ě'),\n",
    "                 (' (miàn)', '(miàn)'), (' (qíng)', '(qíng)'), (' (zi)', '(zi)'),\n",
    "                 (' (jī)', '(jī)'), ('yèlǐ (lǐ)', 'yèlǐ'), (' (zhe)', '(zhe)'),\n",
    "                 (' (cháng)', '(cháng)')):\n",
    "        pinyin = pinyin.replace(x, y).strip()\n",
    "    m = re.match(r\"^([a-zāáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ/ '()]|\\b, |3c |[?]$|……)+$\", pinyin.lower())\n",
    "    if not m: print(pinyin)\n",
    "    assert m\n",
    "    row['Pinyin'] = pinyin\n",
    "\n",
    "    ex = row.get('Examples', row.get('Example', ''))\n",
    "    if row.get('Example2'):\n",
    "        for s in row['Example2'].split('<br>'):\n",
    "            s = s.strip()\n",
    "            if len(s) >= 2 and cn_score(s) > 0.5:\n",
    "                if ex: ex += '<br>'\n",
    "                ex += s\n",
    "    for x, y in [\n",
    "        (r'(<br>)+', '<br>'), (r'^(<br>)+', ''), (r'(<br>)+$', ''),\n",
    "        (r'([\\t \\u200b\\ufeff\\u00a0])+', ' '),\n",
    "        ('[?]','？'), ('!', '！'), ('([^a-z]),', r'\\1，'),\n",
    "        ('⑴', '(1)'), ('⑵', '(2)'), ('⑶', '(3)'),\n",
    "        (r'\\bA[：:]', 'A:'), (r'\\bB[：:]', 'B:'),\n",
    "        (r' *\\(A\\)[：:]', ' A:'), (r' *\\(B\\)[：:]', ' B:'),\n",
    "        ('，<br>', '，'),\n",
    "        ('？<br>([AB])[：:]', r'？ \\1:'),\n",
    "        ('。<br>([AB])[：:]', r'？ \\1:'),\n",
    "        ('^1[.]', '(1)'), ('<br>2[.]', '<br>(2)'), ('<br>3[.]', '<br>(3)'),\n",
    "        ('lǎoshī.*shēngxiào.<br>', ''),\n",
    "        (r'([\\u4200-\\u9fff]) +([\\u4200-\\u9fff])', r'\\1\\2'),\n",
    "    ]:\n",
    "        ex = re.sub(x, y, ex).strip()\n",
    "    if '(2)' in ex:\n",
    "        ex = re.sub('^[(][12][)]', '', ex).strip()\n",
    "        ex = re.sub('<br>[(][2-9][)]', '\\n', ex)\n",
    "        ex = re.sub('([。！]|和體力) *[(][2-9][)]', r'\\n', ex)\n",
    "        ex = re.sub('<br>', '', ex).strip()\n",
    "        ex = '<br>'.join([x.strip() for x in ex.split('\\n') if x.strip()])\n",
    "    elif '<br>' in ex:\n",
    "        ex = re.sub('<br>', '', ex).strip()\n",
    "    row['Examples'] = ex\n",
    "    if 'Example' in row: del row['Example']\n",
    "    if 'Example2' in row: del row['Example2']\n",
    "\n",
    "    row.setdefault('POS', '')\n",
    "\n",
    "    for col in ['Traditional', 'Pinyin', 'Meaning', 'POS', 'Examples']:\n",
    "        row[col] = row[col].strip()\n",
    "    for col in ['Traditional', 'Pinyin', 'Meaning']:\n",
    "        assert row[col]\n",
    "\n",
    "    text = row['Meaning']\n",
    "    for x, y in [\n",
    "        (r'([\\t \\u200b\\ufeff\\u00a0])+', ' '),\n",
    "        (r'“(.*)’’', r'\"\\1\"'),\n",
    "        ('’', \"'\"), ('‘', \"'\"), ('’', \"'\"), \n",
    "        ('”;', '\";'), ('”[.]', '\".'), ('”=', '\"='), ('“of ”', '\"of\"'), ('“', ' \"'), ('”', '\" '),\n",
    "        ('ﬃ', 'ffi'), ('ﬄ', 'ffl'), ('ﬁ', 'fi'), ('ﬂ', 'fl'), ('ﬀ', 'ff'),\n",
    "        ('…[.]{3}', '...'), ('…', '...'),\n",
    "        (' *[(] *M[:：] *', ' (M: '), (' *： *', ': '), (' *； *', '; '),\n",
    "        ('、 *', ', '), ('  +', ' '), ('（', ' ('), ('）', ') '),\n",
    "        (' [)] , ', '), '), (' ([:;,]) ', r'\\1 '), (' +; *', '; '),\n",
    "        (' ,([a-z])', r', \\1'),\n",
    "        (r'\\.\\.\\., *', '..., '),\n",
    "        ('([a-z)])([,;:])([a-z])', r'\\1\\2 \\3'),\n",
    "        ('([a-z])([(])([a-z])', r'\\1 (\\3'), ('([a-z])([)])([a-z])', r'\\1) \\3'),\n",
    "        ('[(] *', '('), (' *[)]', ')'),\n",
    "        ('！,ah', '! ah'), ('！', '! '), (' *! *', '! '), ('! \"', '!\"'), ('!,', '!, '),\n",
    "        (r'\\(someone~\\)', '(someone)'), (r'\\[foolishly\\]', '(foolishly)'),\n",
    "        ('^N[)] ', ''),\n",
    "        ('ă', 'ǎ'), ('ĕ', 'ě'), ('ĭ', 'ǐ'), ('ŏ', 'ǒ'), ('ŭ', 'ǔ'), ('ǎ', 'ǎ'), ('a\\u030c', 'ǎ'), ('e\\u030c', 'ě'),\n",
    "        ('^[)] *', ''), ('^To ', 'to '), (' +', ' '),\n",
    "        (' *[(]M: 個 *(ge|)[)]$', ''),\n",
    "        ('M: 間zhāng', 'M: 間jiān'),\n",
    "        (r'[(]M: ([\\u4E00-\\uFFFF])[; ]*([a-zāáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ]+)[)]', r'(M: \\1\\2)'),\n",
    "        (r'[(]M: ([\\u4E00-\\uFFFF])[,;/ ]+ *([\\u4E00-\\uFFFF])[)]', r'(M: \\1,\\2)'),\n",
    "        ('M: 片;朵',        'M: 片,朵'),\n",
    "        ('M: 部/輛;liàng',  'M: 部,輛liàng'),\n",
    "        ('M: 個/間jiān',    'M: 個,間jiān'),\n",
    "        ('M: 棟dòng/個',    'M: 棟dòng,個'),\n",
    "        ('M: 盤Pán, 道dào', 'M: 盤pán,道dào'),\n",
    "        ('M: 根gēn, 支',    'M: 根gēn,支'),\n",
    "        ('M: 種/股gǔ',      'M: 種,股gǔ'),\n",
    "        ('M: 張, 幅fú',     'M: 張,幅fú'),\n",
    "        ('M: 塊;份;客',     'M: 塊,份,客'),\n",
    "    ]:\n",
    "        text = re.sub(x, y, text).strip()\n",
    "    m = re.match(r'''^([-a-z0-9āáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ ,.:;!?+&/=()\"'\\u4e00-\\u9fff\\[\\]])+$''', text.lower())\n",
    "    if not m: print(text)\n",
    "    assert m\n",
    "    row['Meaning'] = text\n",
    "\n",
    "    row['Simplified'] = to_simplified(row['Traditional'])\n",
    "\n",
    "    row['Variants'] = ''\n",
    "    if row['ID'] in variants_df.index:\n",
    "        assert variants_df.loc[row['ID'], 'Traditional'] == row['Traditional'], (row, variants_df.loc[row['ID'], 'Traditional'])\n",
    "        row['Variants'] = variants_to_json(variants_df.loc[row['ID'], 'Variants'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b3c4430-8764-44f4-907b-8d23247dbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "errata_df = pd.read_csv('errata.csv', dtype='str').fillna('').set_index(['ID', 'Column'])\n",
    "\n",
    "lower_case_meaning_ids = set('''\n",
    "B1L02-1-24 B1L02-3-02 B1L04-1-23 B1L05-1-01 B1L05-2-06 B1L05-3-04 B1L06-1-06 B1L06-2-02 B1L06-2-11 B1L06-2-14\n",
    "B1L06-2-17 B1L06-3-07 B1L06-3-10 B1L08-1-01 B1L08-1-09 B1L08-1-21 B1L08-2-01 B1L08-2-02 B1L09-1-01 B1L09-2-08\n",
    "B1L09-3-13 B1L14-3-04 B2L01-1-21 B2L02-1-35 B2L02-2-20 B2L02-2-23 B2L02-2-24 B2L04-1-15 B2L04-1-24 B2L04-1-34\n",
    "B2L04-1-35 B2L04-2-30 B2L05-1-15 B2L05-1-29 B2L06-1-33 B2L06-1-37 B2L08-1-01 B2L08-1-03 B2L08-1-21 B2L10-1-32\n",
    "B2L11-1-08 B2L13-1-23 B2L13-1-33 B3L02-2-04 B3L06-1-01 B3L06-1-07 B3L06-1-12 B3L06-1-13 B3L06-1-18 B3L06-1-22\n",
    "B3L06-1-30 B3L06-1-31 B3L06-1-32 B3L06-1-38 B3L06-1-40 B3L06-2-04 B3L06-2-08 B3L06-2-10 B3L06-2-15 B3L06-2-18\n",
    "B3L07-1-03 B3L07-1-11 B3L07-2-17 B3L08-1-08 B3L10-1-23 B3L15-1-41 B4L01-1-01 B4L02-1-28 B4L02-2-24 B4L06-1-04\n",
    "B4L06-2-03 B4L06-2-31 B4L12-1-48 B4L13-1-23 B4L14-1-02 B1L04-1-13\n",
    "'''.split())\n",
    "\n",
    "for row in entries:\n",
    "    for col in row:\n",
    "        key = (row['ID'], col)\n",
    "        if key in errata_df.index:\n",
    "            old, corr = errata_df.loc[key, 'Original'], errata_df.loc[key, 'Corrected']\n",
    "            assert row[col] in (old, corr), (row, col, old, row[col])\n",
    "            row[col] = corr\n",
    "    if row['ID'] in lower_case_meaning_ids:\n",
    "        row['Meaning'] = row['Meaning'][0].lower() + row['Meaning'][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44207596-c78c-4c4f-921f-f09c309ef481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4061, 3753)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(entries)\n",
    "assert list(df.ID) == list(sorted(df.ID))\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Examples', 'Variants']\n",
    "#cols += [c for c in df if c not in cols]\n",
    "df = df[cols].set_index('ID').copy()\n",
    "df.to_csv('modernchinese.csv', index=True)\n",
    "len(df), len(set(df.Traditional))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a10cb7-39ea-444d-b78b-3ad0c95d6dd3",
   "metadata": {},
   "source": [
    "## Expand variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b87306-8e73-4d0f-b26f-b72cdbe4191b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modernchinese-expanded.csv: 4218 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "expanded_rows = []\n",
    "for row in pd.read_csv('modernchinese.csv', dtype='str').fillna('').to_dict(orient='records'):\n",
    "    for var_dict in json.loads(row['Variants'] or '[{}]'):\n",
    "        var = dict(row)\n",
    "        var.update(var_dict)\n",
    "        var.pop('Variants')\n",
    "        assert len(var['Simplified'].split()) == 1  # not empty and not spaces\n",
    "        assert '/' not in var['Simplified']\n",
    "        expanded_rows.append(var)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows)\n",
    "expanded_df.to_csv('modernchinese-expanded.csv', index=False)\n",
    "print('modernchinese-expanded.csv: %d rows\\n' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ab91a0-edc3-4d42-9fe8-31106685f6d7",
   "metadata": {},
   "source": [
    "## Export in pleco format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c6a9318-f3ce-425c-b5a7-3db79fb3fda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export in pleco's flashcards format\n",
    "\n",
    "EAC1_TAG = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uEC99'  # lesson tag color, #00cc99 green\n",
    "EAC1_EX = '\\uEAC1\\uEC00\\uEC05\\uECAA\\uECFF'   # examples, #05aaff blue\n",
    "EAC1_HL = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uECCC'   # term highlight in examples, teal\n",
    "\n",
    "df = pd.read_csv('modernchinese.csv', dtype='str').fillna('')\n",
    "\n",
    "with open('modernchinese-pleco.txt', 'w') as fout:\n",
    "    last_header = ''\n",
    "    for row in df.itertuples():\n",
    "        m = re.match('^B([1-4])L([0-9]{2})-([1-3])-.*', row.ID)\n",
    "        assert m\n",
    "\n",
    "        header = f'//時代華語/Book {m[1]}/L{m[2]}-{m[3]}'\n",
    "        if header != last_header:\n",
    "            fout.write(header + '\\n')\n",
    "            last_header = header\n",
    "\n",
    "        tag = f' {EAC1_TAG}[M{m[1]}L{int(m[2])}]\\uEAC2'  # lesson tag [MxLxx]\n",
    "\n",
    "        defn = f'({row.POS}) {row.Meaning}' if row.POS else row.Meaning\n",
    "        # remove brackets around MW so pleco doesn't enlarge char+brackets\n",
    "        defn = re.sub(r' *[(](M: [^()]+)[)]', r'; \\1', defn)\n",
    "\n",
    "        variants = []\n",
    "        trad_variants = set()\n",
    "        for var_dict in json.loads(row.Variants or '[{}]'):\n",
    "            var = dict(row._asdict())\n",
    "            var.update(var_dict)\n",
    "            variants.append(var)\n",
    "            trad_variants.add(var['Traditional'])\n",
    "\n",
    "        ex = ''\n",
    "        if row.Examples:\n",
    "            # highlight the term in example sentences\n",
    "            ex = row.Examples\n",
    "            assert '\\n' not in ex\n",
    "            masked = trad_variants\n",
    "            found = any(s in ex for s in masked)\n",
    "            if not found and '-sep' in row.POS and len(trad_variants) <= 1:\n",
    "                a, b = row.Traditional[0], row.Traditional[1:]\n",
    "                if ex.count(a) == 1 and ex.count(b) == 1 and ex.index(a) < ex.index(b):\n",
    "                    masked = [a, b]\n",
    "                    found = 2\n",
    "            if found:\n",
    "                ex = re.sub('(%s)' % '|'.join(sorted(masked, key=lambda s: -len(s))),\n",
    "                            f'\\uEAC2{EAC1_HL}\\\\1\\uEAC2{EAC1_EX}', ex)\n",
    "            ex = f'\\uEAB1{EAC1_EX}{ex}\\uEAC2'.replace('<br>', '\\uEAB1')\n",
    "\n",
    "        if set(trad_variants) != set([row.Traditional]):\n",
    "            defn = f'{row.Traditional} [{row.Pinyin}]\\uEAB1{defn}'\n",
    "\n",
    "        trad_seen = set()\n",
    "        for var in variants:\n",
    "            # expand hanzi variants to separate entries, but for pinyin variants emit 'variant (variant)'\n",
    "            trad = var['Traditional']\n",
    "            if trad in trad_seen:\n",
    "                continue\n",
    "            trad_seen.add(trad)\n",
    "\n",
    "            pinyin = var['Pinyin']\n",
    "            pinyin_variants = [x['Pinyin'] for x in variants if x['Traditional'] == var['Traditional']]\n",
    "            assert len(pinyin_variants) <= 2, variants\n",
    "            if len(pinyin_variants) > 1:\n",
    "                pinyin = '%s (%s)' % tuple(pinyin_variants)\n",
    "\n",
    "            simp = var['Simplified']\n",
    "            fout.write(f'{simp}[{trad}]\\t{pinyin}\\t{defn}{tag}{ex}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237008a9-5007-42d2-a960-640fdfd63834",
   "metadata": {},
   "source": [
    "## Generate anki package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80cf7b34-6064-46bb-9579-693dc9081509",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data/media\n",
    "!cp -f ../downloads/fonts/MoeStandardKai.ttf data/media/_MoeStandardKai.ttf\n",
    "\n",
    "import os, glob, pandas as pd\n",
    "\n",
    "for row in pd.read_csv('audio.csv', dtype='str').fillna('').itertuples():\n",
    "    dst = f'data/media/modernchinese-{row.ID}.mp3'\n",
    "    if os.path.exists(f'downloads/shengzi/{row.Source}') and not os.path.exists(dst) and row.OK != '0':\n",
    "        cmd = f\"ffmpeg -v error -i 'downloads/shengzi/{row.Source}' -ss {row.Start} -to {row.End} -c copy -vn -sn -dn -y '{dst}'\"\n",
    "        assert os.system(cmd) == 0, cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "783ebe72-1d7d-4d78-b880-3d24cd09cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'POS', 'Meaning', 'Examples', 'Variants', 'Audio']\n",
    "\n",
    "model = genanki.Model(\n",
    "    1696565462,\n",
    "    'ModernChinese',\n",
    "    fields=[{'name': c} for c in cols],\n",
    "    templates=[{\n",
    "        'name': 'ModernChinese',\n",
    "        'qfmt': open('../dangdai/dangdai-qfmt.html').read(),\n",
    "        'afmt': open('../dangdai/dangdai-afmt.html').read().replace(\n",
    "            '<div>{{Audio}}</div>',\n",
    "            '<div>{{Audio}}</div>\\n<br>\\n<div>{{Examples}}</div>')\n",
    "    }],\n",
    "    css=open('../dangdai/dangdai.css').read(),\n",
    ")\n",
    "\n",
    "deck = genanki.Deck(\n",
    "    1696565463,\n",
    "    name='modernchinese',\n",
    "    description='Modern Chinese vocabulary deck'\n",
    ")\n",
    "\n",
    "audio_df = pd.read_csv('audio.csv', dtype='str').set_index('ID')\n",
    "\n",
    "for row in pd.read_csv('modernchinese.csv', dtype='str').fillna('').to_dict(orient='records'):\n",
    "    row['Audio'] = ''\n",
    "    if int(audio_df.loc[row['ID'], 'OK']) != 0:\n",
    "        row['Audio'] = f\"[sound:modernchinese-{row['ID']}.mp3]\"\n",
    "    note = genanki.Note(\n",
    "        model=model,\n",
    "        fields=[row[c] for c in cols],\n",
    "        guid=genanki.guid_for('modernchinese', row['ID'], row['Traditional']),\n",
    "    )\n",
    "    deck.add_note(note)\n",
    "\n",
    "!rm -f modernchinese.apkg\n",
    "genanki.Package(deck, media_files=glob.glob('data/media/*')).write_to_file('modernchinese.apkg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
