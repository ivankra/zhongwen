{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bbe7e2a-3bbc-4880-ac75-3e00207a3447",
   "metadata": {},
   "source": [
    "##  Whisper experiments to split audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832f3edd-4dbe-4656-aa29-15c030040342",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -e\n",
    "if ! [[ -d downloads/ ]]; then\n",
    "  if [[ -d ../downloads/modernchinese ]]; then ln -s ../downloads/modernchinese downloads; else mkdir -p downloads; fi\n",
    "fi\n",
    "\n",
    "if ! [[ -f downloads/shengzi/B4-16-2-2.mp3 ]]; then\n",
    "  mkdir -p downloads/shengzi\n",
    "  /bin/cp -f downloads/B?L??/B1-00-13.mp3 downloads/shengzi/\n",
    "  /bin/cp -f downloads/B?L??/B?*-?-2.mp3 downloads/shengzi/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e397f5bc-9449-4bd0-a2da-ac806f78699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --default-timeout=10000 openai-whisper\n",
    "!pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacdce0-6265-4921-923a-2ec4850e4e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, json, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import librosa\n",
    "import opencc\n",
    "pd.options.display.max_rows = 1000\n",
    "\n",
    "opencc_tw2s = opencc.OpenCC('tw2s')\n",
    "\n",
    "mc_df = pd.read_csv('modernchinese.tsv', sep='\\t')\n",
    "\n",
    "shengzi_mp3 = list(sorted(glob.glob('downloads/shengzi/B[1234]-*-?-2.mp3')))\n",
    "assert len(shengzi_mp3) == 144\n",
    "shengzi_mp3 = ['downloads/shengzi/B1-00-13.mp3'] + shengzi_mp3\n",
    "\n",
    "cn_numbers = list('零一二三四五六七八九')\n",
    "for x in range(1, 10):\n",
    "    for y in range(10):\n",
    "        cn_numbers.append('%s十%s' % ('' if x == 1 else cn_numbers[x], '' if y == 0 else cn_numbers[y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7588011-1a9c-436b-b2fd-c386a5fab35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "for filename in shengzi_mp3:\n",
    "    json_filename = filename.replace('.mp3', '.json')\n",
    "    if os.path.exists(json_filename):\n",
    "        continue\n",
    "    print(filename)\n",
    "    res = model.transcribe(filename, language='zh')\n",
    "    with open(json_filename, 'w') as f:\n",
    "        json.dump(res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a811b1d-9edf-4178-82c4-25585b686b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "h2py = collections.defaultdict(set)\n",
    "py2h = collections.defaultdict(set)\n",
    "\n",
    "for fn in ['../cedict/cedict.csv', '../dangdai/dangdai.tsv', 'modernchinese.tsv']:\n",
    "    for row in pd.read_csv(fn, sep='\\t' if '.tsv' in fn else ',').itertuples():\n",
    "        h2py[row.Traditional].add(row.Pinyin.lower())\n",
    "        h2py[row.Simplified].add(row.Pinyin.lower())\n",
    "        py2h[row.Pinyin.lower()].add(row.Simplified)\n",
    "        py2h[row.Pinyin.lower()].add(row.Traditional)\n",
    "\n",
    "def find_homonyms(h):\n",
    "    res = set()\n",
    "    for py in h2py[h]:\n",
    "        for hh in py2h[py]:\n",
    "            res.add(hh)\n",
    "    return res\n",
    "\n",
    "find_homonyms('他')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d86df3-5dfc-41f9-b15a-df88a4adf4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_extra = '''\n",
    "三聚不离本行 三句不离本行\n",
    "大兜 大都\n",
    "衣衣 一一\n",
    "'''.strip().split('\\n')\n",
    "transcription_extra = {s.split()[0]: s.split()[1:] for s in transcription_extra}\n",
    "transcription_extra['刀 刀子'] = '刀（子）'\n",
    "transcription_extra['叉 叉子'] = '叉（子）'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c23e37-442d-40f9-b4a3-c5cdf303d9dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_terms = 0\n",
    "tot_rec = 0\n",
    "\n",
    "rec_terms = []\n",
    "\n",
    "def srt_timestamp(seconds):\n",
    "    milliseconds = round(seconds * 1000.0)\n",
    "    hours = milliseconds // 3600000\n",
    "    milliseconds -= hours * 360000\n",
    "    minutes = milliseconds // 60000\n",
    "    milliseconds -= minutes * 60000\n",
    "    seconds = milliseconds // 1000\n",
    "    milliseconds -= seconds * 1000\n",
    "    return f\"{hours}:{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
    "\n",
    "for fidx, filename in enumerate(sorted(glob.glob('downloads/shengzi/*.json'))):\n",
    "    transcribed = json.load(open(filename,'r'))\n",
    "\n",
    "    if 'B1-00-1' in filename:\n",
    "        lesson_part = 'B1L00-1'\n",
    "    else:\n",
    "        m = re.match('B([1234])-(.*)-(.)-2.json', os.path.basename(filename))\n",
    "        assert m, filename\n",
    "        lesson_part = 'B%sL%.2d-%s' % (m[1], int(m[2]), m[3])\n",
    "    terms_df = mc_df[mc_df.ID.str.extract('^(B.L..-.)')[0] == lesson_part]\n",
    "    terms = list(terms_df.Simplified)\n",
    "    term_variants = list(terms_df.Variants.fillna(''))\n",
    "    print('\\n%s %s %s' % (filename, len(terms), terms))\n",
    "\n",
    "    book_idx = 0\n",
    "    book_idx_pref = ''\n",
    "    term_idx = -1\n",
    "    book_line_tokens = 0\n",
    "    book_line_matched = 0\n",
    "    rec = 0\n",
    "\n",
    "    for segment in transcribed['segments']:\n",
    "        text = segment['text']\n",
    "        if book_idx == 0 and len(text) == 2 and text[0] in '生声': continue\n",
    "\n",
    "        text = text.replace('.', ' . ')\n",
    "        prefixed = 0\n",
    "        for skip in range(3):\n",
    "            if book_idx > 0 and book_line_tokens == 0: break\n",
    "            k = book_idx + 1 + skip\n",
    "            if text.startswith(str(k)) or text.startswith(cn_numbers[k]):\n",
    "                if book_idx > 0:\n",
    "                    if book_line_matched == 0:\n",
    "                        print('  [UNMATCHED; NEXT: %s]' % terms[term_idx+1:min(len(terms),term_idx+2)], end='')\n",
    "                    print()\n",
    "                print('%3d) %s; ' % (k, text), end='')\n",
    "                book_idx = k\n",
    "                book_line_tokens = 0\n",
    "                book_line_matched = 0\n",
    "                if text.startswith(str(k)):\n",
    "                    pref = str(k)\n",
    "                else:\n",
    "                    pref = cn_numbers[k]\n",
    "                text = text[len(pref):].strip()\n",
    "                text = re.sub('^[.,。;] *', '', text)\n",
    "                prefixed = 1\n",
    "                break\n",
    "\n",
    "        if text.lower() == 'tracers':\n",
    "            text = 'phrases'\n",
    "        if re.match('(phrases|names)', text.lower().strip()):\n",
    "            print('\\n%s' % text)\n",
    "            book_idx = 0\n",
    "            book_idx_pref = text.upper()[0]\n",
    "            continue\n",
    "\n",
    "        if text:\n",
    "            book_line_tokens += 1\n",
    "            cand_a = term_idx + 1\n",
    "            cand_b = min(len(terms), term_idx + 6)\n",
    "            variants = [text]\n",
    "            variants.append(re.sub(' *[?!？！.。)]$', '', text))\n",
    "            variants.append(opencc_tw2s.convert(text))\n",
    "            variants.extend(transcription_extra.get(text, []))\n",
    "            if term_idx+1 < len(terms):\n",
    "                variants.extend(list(find_homonyms(text)))\n",
    "\n",
    "            found = -1\n",
    "            for var_i, variant in enumerate(variants):\n",
    "                for j in range(cand_a, cand_b):\n",
    "                    if variant == terms[j]:\n",
    "                        found = j\n",
    "                        break\n",
    "                    if term_variants[j] != '':\n",
    "                        for trad in term_variants[j].split(' / '):\n",
    "                            trad = trad.split('[')[0].strip()\n",
    "                            if variant == trad or variant == opencc_tw2s.convert(trad):\n",
    "                                found = j\n",
    "                                break\n",
    "                if found >= 0:\n",
    "                    break\n",
    "\n",
    "            if found >= 0:\n",
    "                term_idx = found\n",
    "                print('%s{%d}' % (text, 1 + term_idx), end='; ')\n",
    "                rec += 1\n",
    "                book_line_matched += 1\n",
    "                row = {\n",
    "                    'ID': terms_df.ID.iloc[term_idx],\n",
    "                    'Traditional': terms_df.Traditional.iloc[term_idx],\n",
    "                    'Simplified': terms_df.Traditional.iloc[term_idx],\n",
    "                    'Pinyin':  terms_df.Pinyin.iloc[term_idx],\n",
    "                    'BookIndex': book_idx_pref + str(book_idx),\n",
    "                    'Source': os.path.basename(filename).replace('.json', '.mp3'),\n",
    "                    'Start': segment['start'],\n",
    "                    'End': segment['end'],\n",
    "                    'Transcribed': text,\n",
    "                    'Prefix': prefixed,\n",
    "                }\n",
    "                rec_terms.append(row)\n",
    "                segment['rec'] = f\"{row['ID']} {row['Traditional']} / {row['Simplified']} [{row['Pinyin']}]\"\n",
    "            else:\n",
    "                print(text, end='; ')\n",
    "                pass\n",
    "    print('')\n",
    "\n",
    "    print('%d/%d found' % (rec, len(terms)))\n",
    "    tot_terms  += len(terms)\n",
    "    tot_rec += rec\n",
    "\n",
    "    with open(filename.replace('.json', '.srt'), 'w') as fsrt:\n",
    "        for i, segment in enumerate(transcribed['segments']):\n",
    "            #fsrt.write(f\"{i+1}\\n{srt_timestamp(segment['start'])} --> {srt_timestamp(segment['end'])}\\n\")\n",
    "            fsrt.write('%d\\n%.2f --> %.2f\\n' % (i+1, segment['start'], segment['end']))\n",
    "            fsrt.write(f\"{segment['text'].replace('-->', '->').strip()}\\n\")\n",
    "            if 'rec' in segment:\n",
    "                fsrt.write('{%s}\\n' % segment['rec'])\n",
    "            fsrt.write('\\n')\n",
    "\n",
    "rec_df = pd.DataFrame(rec_terms)\n",
    "rec_df.to_csv('whisper.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e44d36-4651-4455-bb2c-34a7e3cd0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total: %d/%d found, %d missing' % (tot_rec, tot_terms, tot_terms-tot_rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151727d6-58d3-46b5-805d-8d43d2ff797f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MARGIN = 0.25\n",
    "\n",
    "wave_filename = ''\n",
    "\n",
    "for row in rec_terms:\n",
    "    if row['Source'] != wave_filename:\n",
    "        wave_filename = row['Source']\n",
    "        wave, sr = librosa.load(f\"downloads/shengzi/{row['Source']}\", sr=None)\n",
    "\n",
    "    seg = wave[int(row['Start']*sr):int(row['End']*sr)]\n",
    "    adj_start = 0\n",
    "    adj_end = row['End'] - row['Start']\n",
    "\n",
    "    sil = librosa.effects.split(seg, ref=np.max(wave), frame_length=5000, hop_length=1000)\n",
    "    #print(row['ID'], len(sil), 'p=',row['Prefix'], '%.2f' % row['Start'], '\\t', sil.tolist())\n",
    "\n",
    "    if row['Prefix'] == 1:\n",
    "        if len(sil) == 0:\n",
    "            print('%s no segments\\n' % row['ID'])\n",
    "        elif len(sil) == 1:\n",
    "            adj_start = sil[0][0]/sr - MARGIN\n",
    "            adj_end = sil[0][1]/sr + MARGIN\n",
    "        else:\n",
    "            adj_start = sil[1][0]/sr - MARGIN\n",
    "            adj_end = sil[1][1]/sr + MARGIN\n",
    "    else:\n",
    "        if len(sil) > 0:\n",
    "            adj_start = sil[0][0]/sr - MARGIN\n",
    "            adj_end = sil[0][1]/sr + MARGIN\n",
    "\n",
    "    row['AdjStart'] = row['Start'] + adj_start\n",
    "    row['AdjEnd'] = row['Start'] + adj_end\n",
    "\n",
    "rec_df = pd.DataFrame(rec_terms)\n",
    "rec_df.to_csv('whisper.csv', index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe27bb65-4b3c-4ca4-91b5-ee2a650df3e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "import pandas as pd\n",
    "\n",
    "!mkdir -p data/audio\n",
    "\n",
    "for row in pd.read_csv('whisper.csv', dtype='str').itertuples():\n",
    "    dst = f'data/audio/modernchinese-{row.ID}.mp3'\n",
    "    assert os.path.exists(f'downloads/shengzi/{row.Source}')\n",
    "    cmd = f\"ffmpeg -v error -i 'downloads/shengzi/{row.Source}' -ss {row.AdjStart} -to {row.AdjEnd} -c copy -vn -sn -dn -y '{dst}'\"\n",
    "    print(cmd)\n",
    "    ret = os.system(cmd)\n",
    "    assert ret == 0, cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f90eaf-2868-454b-affd-3fbc81ab7458",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "recent = []\n",
    "for row in rec_terms:\n",
    "    #res = model.transcribe(dst, initial_prompt='繁體中文', prepend_punctuations='', append_punctuations='', language='zh')\n",
    "    recent.append(re.sub('[/（].*', '', row['Traditional']).strip())\n",
    "    if len(recent) > 5:\n",
    "        recent = recent[-5:]\n",
    "    dst = f\"data/audio/modernchinese-{row['ID']}.mp3\"\n",
    "    r = list(recent)\n",
    "    random.shuffle(r)\n",
    "    prompt = 'Glossary: ' + ', '.join(r) + '. 繁體中文'\n",
    "    res = model.transcribe(dst, initial_prompt=prompt, prepend_punctuations='', append_punctuations='', language='zh')\n",
    "    text = re.sub('[。？！/?!.,（）]', '', res['text'])\n",
    "    row['PostTranscription2'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb07ef-827a-408e-8907-52d0c926d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for row in rec_terms:\n",
    "    if (row['PostTranscription'] == row['Traditional'] or\n",
    "        row['PostTranscription'] in row['Traditional'].split('/') or\n",
    "        row['PostTranscription'] == re.sub('[（） ]|/.*', '', row['Traditional']) or\n",
    "        row['PostTranscription'] == re.sub('（.*）', '', row['Traditional'])\n",
    "       ):\n",
    "        # or row['Traditional'] in find_homonyms(row['PostTranscription']):\n",
    "        row['Flagged'] = 0\n",
    "    else:\n",
    "        k += 1\n",
    "        row['Flagged'] = 1\n",
    "        #print(row['Traditional'], '\\t', row['PostTranscription'])\n",
    "    if 'PostTranscription2' in row:\n",
    "        row.pop('PostTranscription2')\n",
    "\n",
    "print('%d flagged' % k)\n",
    "\n",
    "rec_df = pd.DataFrame(rec_terms)\n",
    "rec_df.Simplified = list(mc_df.set_index('ID').loc[rec_df.ID].Simplified)\n",
    "rec_df.to_csv('whisper.csv', index=False, float_format='%.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e1273-ff24-4df7-9d9a-7d701e7b837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('whisper.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291647cc-1b4a-43d0-b0ee-d3ebf99575c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = pd.concat([mc_df[['ID', 'Traditional', 'Simplified', 'Pinyin']].set_index('ID'),\n",
    "                      pd.read_csv('whisper.csv').set_index('ID')[[\n",
    "                          'BookIndex', 'Source', 'AdjStart', 'AdjEnd', 'PostTranscription', 'Flagged'\n",
    "                      ]]], axis=1)\n",
    "audio_df['OK'] = list(map(int, audio_df.Flagged.notnull() & (audio_df.Flagged == 0)))\n",
    "audio_df = audio_df.rename(columns={'AdjStart': 'Start', 'AdjEnd': 'End', 'PostTranscription': 'Transcribed'}).drop(columns=['Flagged'])\n",
    "pp = audio_df.index.str.extract('(B.L..-[0-9])')[0]\n",
    "audio_df['Source'] = list(pp.map(audio_df.reset_index().groupby(pp).Source.first().to_dict()))\n",
    "audio_df.to_csv('audio.csv')\n",
    "audio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a920f41-ae08-4d1c-99da-0e67aef422f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "anki_flagged=list(sorted(set(pd.read_csv('sel.txt', sep='\\t', comment='#',header=None)[0])))\n",
    "len(anki_flagged)\n",
    "audio_df.loc[anki_flagged][lambda X: X.OK==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77038dd7-93c8-46b4-976f-f355098d2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(audio_df.OK == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663a20a-39be-4f38-8b68-50e9552250f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os, pandas as pd\n",
    "\n",
    "!mkdir -p data/audio\n",
    "for row in pd.read_csv('audio.csv', dtype='str').fillna('').itertuples():\n",
    "    dst = f'data/audio/modernchinese-{row.ID}.mp3'\n",
    "    assert os.path.exists(f'downloads/shengzi/{row.Source}')\n",
    "    if row.Start:\n",
    "        cmd = f\"ffmpeg -v error -i 'downloads/shengzi/{row.Source}' -ss {row.Start} -to {row.End} -c copy -vn -sn -dn -y '{dst}'\"\n",
    "        assert os.system(cmd) == 0, cmd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927139c3-1957-4d14-ac76-0433bd8e3cbd",
   "metadata": {},
   "source": [
    "## Whisper session 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed3f4eb-a105-47ff-ab30-6c5404ec0525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Widen segments\n",
    "\n",
    "MARGIN = 0.25\n",
    "wave_filename = ''\n",
    "\n",
    "audio_df = pd.read_csv('audio.csv', dtype='str').set_index('ID')\n",
    "\n",
    "for row in audio_df.reset_index().to_dict(orient='records'):\n",
    "    if row['OK'] != '1':\n",
    "        continue\n",
    "\n",
    "    if row['Source'] != wave_filename:\n",
    "        wave_filename = row['Source']\n",
    "        print(wave_filename)\n",
    "        wave, sr = librosa.load(f\"downloads/shengzi/{row['Source']}\", sr=None)\n",
    "        splits = librosa.effects.split(wave, frame_length=5000, hop_length=1000).tolist()\n",
    "\n",
    "    t1, t2 = float(row['Start']), float(row['End'])\n",
    "    t1, t2 = int(t1*sr), int(t2*sr)\n",
    "    match = [s for s in splits if t1 <= s[0] <= t2 or t1 <= s[1] <= t2 or s[0] <= t1 <= s[1] or s[0] <= t2 <= s[1]]\n",
    "    if len(match) != 1:\n",
    "        print('Multiple splits: ', row, t1, t2, match)\n",
    "\n",
    "    a = '%.2f' % max(0.0, match[0][0]/sr - MARGIN)\n",
    "    b = '%.2f' % max(0.0, match[-1][1]/sr + MARGIN)\n",
    "\n",
    "    if abs(float(row['Start']) - float(a)) > 0.1 or abs(float(row['End']) - float(b)) > 0.1:\n",
    "        print(row['ID'], row['Traditional'], row['Pinyin'], row['Start'], row['End'], '-->', a, b)\n",
    "        audio_df.loc[row['ID'], 'Start'] = a\n",
    "        audio_df.loc[row['ID'], 'End'] = b\n",
    "        audio_df.loc[row['ID'], 'OK'] = '2'\n",
    "\n",
    "audio_df.to_csv('audio2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9989bda5-fcf3-4b95-8744-99fd01562830",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = pd.read_csv('audio.csv').set_index('ID')\n",
    "\n",
    "records = audio_df.reset_index().to_dict(orient='records')\n",
    "id_to_range = {}\n",
    "\n",
    "for i, row in enumerate(records):\n",
    "    t1 = 0\n",
    "    j = i\n",
    "    while j > 0 and records[j-1]['Source'] == records[j]['Source']:\n",
    "        j -= 1\n",
    "        if records[j]['OK'] == 1:\n",
    "            t1 = records[j]['End'] + 0.1\n",
    "            break\n",
    "\n",
    "    t2 = 9999\n",
    "    j = i\n",
    "    while j+1 < len(records) and records[j+1]['Source'] == records[j]['Source']:\n",
    "        j += 1\n",
    "        if records[j]['OK'] == 1:\n",
    "            t2 = records[j]['Start'] - 0.1\n",
    "            break\n",
    "\n",
    "    id_to_range[row['ID']] = (t1, t2)\n",
    "\n",
    "!mkdir -p downloads/shengzi-splits\n",
    "splits_mp = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabceca8-600d-439b-a8d2-9eebad1ff034",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "audio_df = pd.read_csv('audio.csv').set_index('ID')\n",
    "\n",
    "MARGIN = 0.25\n",
    "wave_filename = ''\n",
    "\n",
    "for row in audio_df.reset_index().to_dict(orient='records'):\n",
    "    if row['OK'] != 0:\n",
    "        continue\n",
    "\n",
    "    if row['Source'] != wave_filename:\n",
    "        wave_filename = row['Source']\n",
    "        wave, sr = librosa.load(f\"downloads/shengzi/{row['Source']}\", sr=None)\n",
    "        splits = librosa.effects.split(wave, top_db=40, frame_length=2000, hop_length=1000).tolist()\n",
    "\n",
    "    t1, t2 = id_to_range[row['ID']]\n",
    "    t1 -= 2\n",
    "    t2 += 30\n",
    "    t1, t2 = int(t1*sr), int(t2*sr)\n",
    "    cand_splits = [s for s in splits if t1 <= s[0] <= t2 or t1 <= s[1] <= t2 or s[0] <= t1 <= s[1] or s[0] <= t2 <= s[1]]\n",
    "\n",
    "    for a, b in cand_splits:\n",
    "        a = '%.2f' % max(0.0, a/sr - MARGIN)\n",
    "        b = '%.2f' % max(0.0, b/sr + MARGIN)\n",
    "        dst = f\"downloads/shengzi-splits/{row['Source']}:{a}:{b}.mp3\"\n",
    "        if not os.path.exists(dst):\n",
    "            cmd = f\"ffmpeg -v error -i 'downloads/shengzi/{row['Source']}' -ss {a} -to {b} -c copy -vn -sn -dn -y '{dst}'\"\n",
    "            #print(cmd)\n",
    "            assert os.system(cmd) == 0, cmd\n",
    "\n",
    "        if dst not in splits_mp:\n",
    "            splits_mp[dst] = {'Split': dst, 'Source': row['Source'], 'a': a, 'b': b, 'ids': []}\n",
    "        splits_mp[dst]['ids'].append(row['ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18a400-0f96-4252-8f9c-eafc0e0d2405",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "mc_df = pd.read_csv('modernchinese.tsv', sep='\\t').set_index('ID').fillna('')\n",
    "\n",
    "for split_filename, split in splits_mp.items():\n",
    "    if type(split['ids']) is list:\n",
    "        split['ids'] = ' '.join(split['ids'])\n",
    "\n",
    "for split_filename, split in splits_mp.items():\n",
    "    if 'Transcribed' in split:\n",
    "        continue\n",
    "\n",
    "    terms = []\n",
    "    for id in split['ids'].split():\n",
    "        if mc_df.loc[id, 'Variants']:\n",
    "            for variant in mc_df.loc[id, 'Variants'].split(' / '):\n",
    "                terms.append(variant.split()[0])\n",
    "        else:\n",
    "            terms.append(mc_df.loc[id, 'Traditional'])\n",
    "    random.shuffle(terms)\n",
    "    prompt = 'Glossary: ' + ', '.join(terms) + '. 繁體中文'\n",
    "    res = model.transcribe(split_filename, initial_prompt=prompt, prepend_punctuations='', append_punctuations='', language='zh')\n",
    "    text = re.sub('[。？！/?!.,（）]', '', res['text'])\n",
    "    print(split, prompt); print(text)\n",
    "    split['Transcribed'] = text\n",
    "    pd.DataFrame(list(splits_mp.values())).to_csv('whisper-splits.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe46360a-21ba-42ab-a8e8-fa74ebc6ff23",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = pd.read_csv('audio.csv', dtype='str').fillna('').set_index('ID')\n",
    "mc_df = pd.read_csv('modernchinese.tsv', sep='\\t').fillna('').set_index('ID')\n",
    "\n",
    "for split_filename in sorted(splits_mp.keys()):\n",
    "    split = splits_mp[split_filename]\n",
    "    for term_id in split['ids'].split():\n",
    "        if audio_df.loc[term_id, 'OK'] == '0' and \\\n",
    "           (split['Transcribed'] == audio_df.loc[term_id, 'Traditional'] or  \\\n",
    "            ' '+split['Transcribed']+' ' in ' '+mc_df.loc[term_id, 'Variants']+' '):\n",
    "            audio_df.loc[term_id, 'Transcribed'] = split['Transcribed']\n",
    "            audio_df.loc[term_id, 'Start'] = split['a']\n",
    "            audio_df.loc[term_id, 'End'] = split['b']\n",
    "            assert audio_df.loc[term_id, 'Source'] == split['Source']\n",
    "            audio_df.loc[term_id, 'OK'] = '3'\n",
    "\n",
    "audio_df.to_csv('audio.csv')\n",
    "len(audio_df[audio_df.OK == '0'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
