{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c325e439-1711-4fde-b235-d44737f0187f",
   "metadata": {},
   "source": [
    "# TBCL parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b408022-558b-48ca-bc62-65a95f308751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, glob, requests, io, urllib, json\n",
    "import pandas as pd\n",
    "import opencc\n",
    "\n",
    "pd.options.display.max_rows = 2000\n",
    "\n",
    "opencc_tw2s = opencc.OpenCC('tw2s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a21d089-8a49-4d85-9fe4-9074c378efd4",
   "metadata": {},
   "source": [
    "Download files from TBCL home page: https://coct.naer.edu.tw/download/tech_report/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4006e880-016e-4064-9c68-86db698c0ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "![[ ! -d downloads && -d ../downloads/tbcl ]] && ln -s ../downloads/tbcl downloads\n",
    "!mkdir -p downloads\n",
    "\n",
    "if not os.path.exists('downloads/.done'):\n",
    "    home_url = 'https://coct.naer.edu.tw/download/tech_report/'\n",
    "    resp = requests.get(home_url).content.decode('utf-8')\n",
    "    for url in sorted(re.findall('<a href=\"([^\"]+[.](?:xlsx|docx))\"', resp)):\n",
    "        url = os.path.join(home_url, url)\n",
    "        !cd downloads && wget -nc \"{url}\"\n",
    "    !chmod a-w downloads/*.xlsx; touch downloads/.done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789b97a5-dd71-4658-a388-a67edd7ac808",
   "metadata": {},
   "source": [
    "Symlinks for convenience and checksums:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea40a07f-71a8-4786-8d2f-515c7ab61a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5e92ac49c5bb203e16fea29c53a2b2cb790033fb332a699c7689adde21528b8f  affixes.xlsx\n",
      "6329e2516c5dbe416b85f6a94d200ebe95493f24f233a63dc10d85aa257a088f  chars.xlsx\n",
      "b6ce3747a06c8482ce5f4059689463de01a45d2b78707feb85917404ccffae62  glossary.xlsx\n",
      "c587989cf89992d55d97a2f932289ef071648ca80339ccfaff7bb823914e5bcf  grammar.xlsx\n",
      "cb16dcd262eb3e499273f972c9a3a404c40042a7def35631fc40b2a64dd50eb0  words.xlsx\n",
      "b6ce3747a06c8482ce5f4059689463de01a45d2b78707feb85917404ccffae62  臺灣華語文能力基準基礎詞彙表_111-09-20.xlsx\n",
      "6329e2516c5dbe416b85f6a94d200ebe95493f24f233a63dc10d85aa257a088f  臺灣華語文能力基準漢字表_111-09-20.xlsx\n",
      "cb16dcd262eb3e499273f972c9a3a404c40042a7def35631fc40b2a64dd50eb0  臺灣華語文能力基準詞語表_111-11-14.xlsx\n",
      "c587989cf89992d55d97a2f932289ef071648ca80339ccfaff7bb823914e5bcf  臺灣華語文能力基準語法點表_112-01-04.xlsx\n",
      "5e92ac49c5bb203e16fea29c53a2b2cb790033fb332a699c7689adde21528b8f  臺灣華語文能力基準類詞綴表_111-09-20.xlsx\n"
     ]
    }
   ],
   "source": [
    "%%bash -e\n",
    "cd downloads\n",
    "chmod a-w *.xlsx *.ods *.pdf\n",
    "ln -sf '臺灣華語文能力基準詞語表_111-11-14.xlsx' words.xlsx\n",
    "ln -sf '臺灣華語文能力基準詞語表_111-11-14.ods' words.ods\n",
    "ln -sf '臺灣華語文能力基準漢字表_111-09-20.docx' chars.docx\n",
    "ln -sf '臺灣華語文能力基準漢字表_111-09-20.xlsx' chars.xlsx\n",
    "ln -sf '臺灣華語文能力基準類詞綴表_111-09-20.docx' affixes.docx\n",
    "ln -sf '臺灣華語文能力基準類詞綴表_111-09-20.xlsx' affixes.xlsx\n",
    "ln -sf '臺灣華語文能力基準語法點表_112-01-04.xlsx' grammar.xlsx\n",
    "ln -sf '臺灣華語文能力基準語法點表_112-01-04.docx' grammar.docx\n",
    "ln -sf '臺灣華語文能力基準基礎詞彙表_111-09-20.xlsx' glossary.xlsx\n",
    "sha256sum *.xlsx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f2c66-8e66-4000-bf8e-06073ce37676",
   "metadata": {},
   "source": [
    "## Parse wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4d891e-73db-4fdb-ada0-c428284143e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glossary.csv: 1518 rows\n"
     ]
    }
   ],
   "source": [
    "glossary_df = pd.read_excel('downloads/glossary.xlsx').rename(columns={\n",
    "    '序號': 'ID',\n",
    "    '詞語': 'Traditional',\n",
    "    '注音': 'Zhuyin',\n",
    "    '漢拼': 'Pinyin',\n",
    "    '詞類/性質': 'POS',\n",
    "    '詞彙英譯': 'Meaning',\n",
    "    '語義/義項': 'Meaning2',\n",
    "    '用法-常用搭配詞': 'Compounds',\n",
    "    '例句': 'Examples',\n",
    "    '級別': 'Level',\n",
    "})\n",
    "assert list(glossary_df.ID - 1) == list(glossary_df.index)\n",
    "glossary_df['Level'] = glossary_df.Level.str.extract('^第([1-7][*]?)級$')[0]\n",
    "assert sum(glossary_df.Level.isnull()) == 0\n",
    "glossary_df.to_csv('glossary.csv', index=False)\n",
    "print('glossary.csv: %d rows' % len(glossary_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3779144e-46c4-4ebe-9b67-9954ef91d4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unjoined glossary vocab: 1應該/應 1臺灣/台灣 3韓國 2德國 1車/車子 1還 1美國 3亞洲 1日本 1法國 3歐洲 3月台/月臺 1中國 2罐 1英國 3義大利 2瓶子/瓶瓶 3非洲 3美洲\n",
      "tbcl.csv: 14425 rows\n"
     ]
    }
   ],
   "source": [
    "VARIANTS_EXC = {\n",
    "    ('姊姊/姐姐/姊/姐', 'jiějie/jiě'): [['姊姊','jiějie'], ['姐姐','jiějie'], ['姊','jiě'], ['姐','jiě']],\n",
    "    ('那/那裡/那裏/那兒', 'nà/nàlǐ/nàr'): [['那', 'nà'], ['那裡', 'nàlǐ'], ['那裏', 'nàlǐ'], ['那兒', 'nàr']],\n",
    "    ('這/這裡/這裏/這兒', 'zhè/zhèlǐ/zhèr'): [['這', 'zhè'], ['這裡', 'zhèlǐ'], ['這裏', 'zhèlǐ'], ['這兒', 'zhèr']],\n",
    "    ('手錶/手表/錶/表', 'shǒubiǎo/biǎo'): [['手錶', 'shǒubiǎo'], ['手表', 'shǒubiǎo'], ['錶', 'biǎo'], ['表', 'biǎo']],\n",
    "    ('新台幣/新臺幣/台幣/臺幣', 'xīntáibì/táibì'): [['新台幣', 'xīntáibì'], ['新臺幣', 'xīntáibì'], ['台幣', 'táibì'], ['臺幣', 'táibì']],\n",
    "    ('慾望/欲望/慾', 'yùwàng/yù'): [['慾望', 'yùwàng'], ['欲望', 'yùwàng'], ['慾', 'yù']],\n",
    "    ('侄子/姪子/侄兒/姪兒', 'zhízi/zhír'): [['侄子', 'zhízi'], ['姪子', 'zhízi'], ['侄兒', 'zhír'], ['姪兒', 'zhír']],\n",
    "    ('嘴脣/嘴唇/脣/唇', 'zuǐchún/chún'): [['嘴脣', 'zuǐchún'], ['嘴唇', 'zuǐchún'], ['脣', 'chún'], ['唇', 'chún']],\n",
    "    ('沒(有)用', 'méi(yǒu)yòng'): [['沒用', 'méiyòng'], ['沒有用', 'méiyǒuyòng']],\n",
    "    ('一邊(兒)', 'yìbiān(r)'): [['一邊', 'yìbiān'], ['一邊兒', 'yìbiānr']],\n",
    "}\n",
    "\n",
    "def get_variants(vocab, pinyin):\n",
    "    vocab = vocab.strip()\n",
    "    pinyin = re.sub(' */ *', '/', pinyin.strip())\n",
    "\n",
    "    ps = re.sub('[^()/]', '', pinyin)\n",
    "    vs = re.sub('[^()/]', '', vocab)\n",
    "    if ps == '' and vs == '':\n",
    "        return []\n",
    "\n",
    "    if (vocab, pinyin) in VARIANTS_EXC:\n",
    "        return VARIANTS_EXC[(vocab, pinyin)]\n",
    "\n",
    "    if vs == '' and ps:\n",
    "        assert set(ps) == {'/'}\n",
    "        return [[vocab, p.strip()] for p in pinyin.split('/')]\n",
    "\n",
    "    if vs and ps == '':\n",
    "        assert set(vs) == {'/'}\n",
    "        assert len(set(map(len, vocab.split('/')))) == 1, vocab  # all terms same length\n",
    "        return [[v, pinyin] for v in vocab.split('/')]\n",
    "\n",
    "    assert vs == ps and set(ps) == {'/'}, (vocab, pinyin)\n",
    "    return [[v.strip(), p.strip()] for (v, p) in zip(vocab.split('/'), pinyin.split('/'))]\n",
    "\n",
    "def get_variants_str(vocab, pinyin):\n",
    "    variants = get_variants(vocab, pinyin)\n",
    "    return json.dumps(variants, ensure_ascii=False) if variants else ''\n",
    "\n",
    "def fix_traditional(s):\n",
    "    # number suffixes for different pronunciations, +3 weird duplicate entries 空檔 道 來往\n",
    "    s = re.sub('[0-9]', '', s)\n",
    "    s = re.sub('／', '/', s)\n",
    "    assert re.match('^[\\u4E00-\\u9FFF/()]+$', s), (row, s)\n",
    "    return s\n",
    "\n",
    "PINYIN_MP = {\n",
    "    ('nǚér', '女兒'): \"nǚ'ér\",\n",
    "    ('wǎnān', '晚安'): \"wǎn'ān\",\n",
    "    ('zǎoān', '早安'): \"zǎo'ān\",\n",
    "    ('kěài', '可愛'): \"kě'ài\",\n",
    "    ('xiǎpéngyǒu', '小朋友'): 'xiǎopéngyǒu',\n",
    "    ('dáàn', '答案'): \"dá'àn\",\n",
    "    ('jú', '橘子'): 'júzi',\n",
    "    ('pèngchù', '碰觸/觸碰'): 'pèngchù/chùpèng',\n",
    "    ('wányèr', '玩意兒'): \"wányìr\",\n",
    "    ('qīněr', '親耳'): \"qīn'ěr\",\n",
    "    ('yāgēr', '壓根兒'): \"yāgēnr\",\n",
    "    ('yìdiǎn/yìdiǎndiǎn/yìdiǎr', '一點/一點點/一點兒'): 'yìdiǎn/yìdiǎndiǎn/yìdiǎnr',\n",
    "    ('xiáchí', '挾持'): 'xiéchí', #https://dict.revised.moe.edu.tw/dictView.jsp?ID=105985&word=%E6%8C%BE%E6%8C%81\n",
    "}\n",
    "\n",
    "def fix_pinyin(py, trad=''):\n",
    "    for x, y in ['ɑa', (' */ *', '/'), (r'\\s+', ' '), (' */$', ''), ('^/ *', '')]:\n",
    "        py = re.sub(x, y, py).strip()\n",
    "    if (py.replace(' ', ''), trad) in PINYIN_MP:\n",
    "        return PINYIN_MP[(py.replace(' ', ''), trad)]\n",
    "    # Pinyin spaces are not meaningul in TBCL lists, mostly just syllable spaces there.\n",
    "    # Remove to make more mergeable with TOCFL. Also no upper letters.\n",
    "    assert py == py.lower() and \"'\" not in py\n",
    "    merged = ''\n",
    "    for part in py.split():\n",
    "        if merged and merged[-1] not in '/()' and part[0] in 'aeoāáǎàēéěèōóǒò':\n",
    "            merged += \"'\"\n",
    "        merged += part\n",
    "    py = merged\n",
    "    assert re.match(\"^[a-zāáǎàēéěèīíǐìōóǒòūúǔùüǘǚǜ/()']+$\", py), (py, repr(py))\n",
    "    return py\n",
    "\n",
    "def to_simplified(trad):\n",
    "    simp = opencc_tw2s.convert(trad)\n",
    "    simp = simp.replace('擡', '抬') # opencc bug\n",
    "    simp = simp.replace('砲', '炮') # TGH char\n",
    "    return simp\n",
    "\n",
    "\n",
    "df = pd.read_excel('downloads/words.xlsx').rename(columns={\n",
    "    '序號': 'ID',\n",
    "    '詞語': 'Traditional',\n",
    "    '等別': 'Grade',\n",
    "    '級別': 'Level',\n",
    "    '情境': 'Context',\n",
    "    '書面字頻(每百萬字)': 'WritingFreq',\n",
    "    '口語字頻(每百萬字)': 'SpeakingFreq',\n",
    "    '簡編本系統號': 'MOE', # MOE dict IDs, https://dict.concised.moe.edu.tw/dictView.jsp?ID=.\n",
    "    '參考注音': 'Zhuyin',\n",
    "    '參考漢語拼音': 'Pinyin'\n",
    "})\n",
    "\n",
    "assert list(df.ID - 1) == list(df.index)\n",
    "df = df.drop(columns=['Grade'])\n",
    "\n",
    "df['Level'] = df.Level.str.extract('^第([1-7][*]?)級$')[0]\n",
    "assert sum(df.Level.isnull()) == 0\n",
    "\n",
    "df['glossary_key'] = (df.Level.str.slice(0, 1) + df.Traditional)\n",
    "glossary_df = glossary_df.fillna('')\n",
    "glossary_df['glossary_key'] = (glossary_df.Level.str.slice(0, 1) + glossary_df.Traditional)\n",
    "glossary_mp = glossary_df.assign(idx=glossary_df.index).groupby('glossary_key').idx.apply(list)\n",
    "\n",
    "df['Traditional'] = df.Traditional.map(fix_traditional)\n",
    "df.insert(2, 'Simplified', df.Traditional.map(to_simplified))\n",
    "df['Pinyin'] = [fix_pinyin(row.Pinyin, row.Traditional) for row in df.itertuples()]\n",
    "df['Variants'] = [get_variants_str(row.Traditional, row.Pinyin) for row in df.itertuples()]\n",
    "df['MOE'] = df['MOE'].str.replace(\"'\", '\"')\n",
    "\n",
    "for row in df.itertuples():\n",
    "    variants = [v for v,p in json.loads(row.Variants)] if row.Variants else [row.Traditional]\n",
    "    for v, ids in json.loads(row.MOE):\n",
    "        assert v in variants\n",
    "\n",
    "# Join with vocab_df\n",
    "for col in ['POS', 'Meaning', 'Compounds', 'Examples']:\n",
    "    df[col] = ''\n",
    "    for row in df.itertuples():\n",
    "        text = [glossary_df.loc[i, col] for i in glossary_mp.get(row.glossary_key, [])]\n",
    "        text = [s.strip() for s in text if s.strip()]\n",
    "        assert ' / ' not in ''.join(text), text\n",
    "        if not text: continue\n",
    "        dedup = []\n",
    "        for s in text:\n",
    "            if s not in dedup: dedup.append(s)\n",
    "        text = ' / '.join(dedup)\n",
    "        if col == 'POS':\n",
    "            text = text.replace(' ', '')\n",
    "            text = text.replace('Phrase', 'Ph')\n",
    "        elif col == 'Compounds':\n",
    "            text = text.replace(';', '')\n",
    "            text = text.replace(' / ', '，').split('，')\n",
    "            dedup = []\n",
    "            for s in text:\n",
    "                if s not in dedup: dedup.append(s)\n",
    "            text = '，'.join(text)\n",
    "            if text: text += '。'\n",
    "        for x, y in [(' *[(] +', ' ('), (' +[)]', ')'), (' +/ +', ' / ')]:\n",
    "            text = re.sub(x, y, text).strip()\n",
    "        assert '\\n' not in text\n",
    "        df.loc[row.Index, col] = text.strip()\n",
    "\n",
    "print('Unjoined glossary vocab: %s' % ' '.join(set(glossary_df.glossary_key) - set(df.glossary_key)))\n",
    "df = df.drop(columns=['glossary_key'])\n",
    "\n",
    "df.to_csv('tbcl.csv', index=False)\n",
    "print('tbcl.csv: %d rows' % len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f1de529-7e4d-4afb-a775-5417ced153ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tbcl-expanded.csv: 14868 entries\n"
     ]
    }
   ],
   "source": [
    "# Generate version with variants expanded.\n",
    "\n",
    "expanded_rows = []\n",
    "for row in df.fillna('').to_dict(orient='records'):\n",
    "    row['Simplified'] = to_simplified(row['Traditional'])\n",
    "    if not row['Variants']:\n",
    "        expanded_rows.append(row)\n",
    "    else:\n",
    "        for trad, pinyin in json.loads(row['Variants']):\n",
    "            row_v = dict(row)\n",
    "            row_v['Traditional'] = trad\n",
    "            row_v['Simplified'] = to_simplified(trad)\n",
    "            row_v['Pinyin'] = pinyin\n",
    "            expanded_rows.append(row_v)\n",
    "            moe = [x for x in json.loads(row['MOE']) if x[0] == trad]\n",
    "            row_v['MOE'] = json.dumps(moe, ensure_ascii=False)\n",
    "\n",
    "expanded_df = pd.DataFrame(expanded_rows).drop(columns=['Variants'])\n",
    "expanded_df.to_csv('tbcl-expanded.csv', index=False)\n",
    "print('tbcl-expanded.csv: %d entries' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48ca61c0-a6e9-4c44-bf7e-189ac0b98741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 852010 Nov  4 14:58 tbcl-pleco.txt\n"
     ]
    }
   ],
   "source": [
    "EAC1_EX  = '\\uEAC1\\uEC00\\uEC05\\uECAA\\uECFF'  # examples, #05aaff blue\n",
    "EAC1_HL  = '\\uEAC1\\uEC00\\uEC00\\uECCC\\uECCC'  # term highlight in examples, teal\n",
    "\n",
    "tbcl_indexed_df = pd.read_csv('tbcl.csv', dtype='str').fillna('').set_index('ID')\n",
    "\n",
    "with open('tbcl-pleco.txt', 'w') as fout:\n",
    "    last_level = ''\n",
    "    for row in pd.read_csv('tbcl-expanded.csv', dtype='str').fillna('').itertuples():\n",
    "        level = row.Level\n",
    "        if last_level != level:\n",
    "            last_level = level\n",
    "            fout.write(f'//TBCL/L{level}\\n')\n",
    "        text = f'{row.Simplified}[{row.Traditional}]\\t{row.Pinyin}\\t'\n",
    "        trow = tbcl_indexed_df.loc[row.ID]\n",
    "        if trow.Variants:\n",
    "            text += f'{trow.Traditional} [{trow.Pinyin}] '\n",
    "        if trow.POS:\n",
    "            text += f'({trow.POS}) '\n",
    "        if trow.Meaning:\n",
    "            text += f'{trow.Meaning} '\n",
    "        text += f'[TBCL{level}]'\n",
    "        # Compounds and examples in light blue on separate lines\n",
    "        for ex in [row.Compounds, row.Examples]:\n",
    "            if not ex: continue\n",
    "            text += (\n",
    "                f'\\uEAB1{EAC1_EX}' +\n",
    "                ex.replace(' / ', '\\uEAB1').replace(\n",
    "                    row.Traditional,\n",
    "                    f'\\uEAC2{EAC1_HL}{row.Traditional}\\uEAC2{EAC1_EX}'\n",
    "                ) +\n",
    "                '\\uEAC2'\n",
    "            )\n",
    "        fout.write(text + '\\n')\n",
    "\n",
    "!ls -l tbcl-pleco.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cf03e3-7b61-483b-a1e0-49d072e4c3e6",
   "metadata": {},
   "source": [
    "## Convert other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b88b5e6e-fdbf-4a0e-b41d-febf6e953c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars.csv: 3100 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('downloads/chars.xlsx').rename(columns={\n",
    "    '序號': 'ID',\n",
    "    '漢字': 'Traditional',\n",
    "    '等別': 'Grade',\n",
    "    '級別': 'Level',\n",
    "    '情境': 'Context',\n",
    "    '書面字頻（每百萬字）': 'WritingFreq',\n",
    "    '口語字頻（每百萬字）': 'SpeakingFreq',\n",
    "})\n",
    "\n",
    "assert list(df.ID - 1) == list(df.index)\n",
    "df = df.drop(columns=['Grade'])\n",
    "\n",
    "df['Level'] = df.Level.str.extract('^第([1-7][*]?)級$')[0]\n",
    "assert sum(df.Level.isnull()) == 0\n",
    "\n",
    "df['Traditional'] = df.Traditional.map(fix_traditional)\n",
    "df.to_csv('chars.csv', index=False)\n",
    "print('chars.csv: %d rows' % len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25c6d60d-52dc-4de6-bde3-833404500dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars-expanded.csv: 3133 rows\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "for row in df.to_dict(orient='records'):\n",
    "    for ch in row['Traditional'].split('/'):\n",
    "        row['char'] = ch\n",
    "        rows.append(dict(row))\n",
    "\n",
    "expanded_df = pd.DataFrame(rows)[['char'] + list(df.columns)]\n",
    "expanded_df.to_csv('chars-expanded.csv', index=False)\n",
    "print('chars-expanded.csv: %d rows' % len(expanded_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6551c31f-f48a-4e2f-ae79-e38f5bf78775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grammar.csv: 496 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('downloads/grammar.xlsx').rename(columns={\n",
    "    '序號': 'ID',\n",
    "    '語法點': 'Grammar',\n",
    "    '等別': 'Grade',\n",
    "    '級別': 'Level',\n",
    "    '例句': 'Example',\n",
    "})\n",
    "\n",
    "assert list(df.ID - 1) == list(df.index)\n",
    "df = df.drop(columns=['Grade'])\n",
    "\n",
    "df['Level'] = df.Level.str.extract('^第([1-7][*]?)級$')[0]\n",
    "assert sum(df.Level.isnull()) == 0\n",
    "\n",
    "df.to_csv('grammar.csv', index=False)\n",
    "print('grammar.csv: %d rows' % len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4821a7-b877-4413-8774-5cb2b3964920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "affixes.csv: 73 rows\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel('downloads/affixes.xlsx').rename(columns={\n",
    "    '序號': 'ID',\n",
    "    '類詞綴': 'Affix',\n",
    "    '語法點': 'Grammar',\n",
    "    '級別': 'Level',\n",
    "    '說明': 'Explanation',\n",
    "    '相關詞彙': 'Words',\n",
    "})\n",
    "assert list(df.ID - 1) == list(df.index)\n",
    "\n",
    "df['Level'] = df.Level.str.extract('^第([1-7][*]?)級$')[0]\n",
    "assert sum(df.Level.isnull()) == 0\n",
    "\n",
    "df.to_csv('affixes.csv', index=False)\n",
    "print('affixes.csv: %d rows' % len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2681b-d30d-49b8-a902-78f2209a99be",
   "metadata": {},
   "source": [
    "## Readings check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9458efc4-51ad-4022-8513-61208da0ac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tbcl.csv', dtype='str').fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3d5db96-c576-4406-a032-c14abd57e367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1533', '噢', '噢', '4', '核心詞', '80', '19', '[[\"噢\", [\"44379\"]]]'] :\t 噢 yǔ vs. ['ō']\n",
      "['1572', '阿嬤/阿媽', '阿嬷/阿妈', '4', '1.個人資料', '41', '301', '[]'] :\t 阿嬤 āma vs. ['āmo', 'āmā', 'āmó', 'àmo', 'àmā', 'àmó', 'ēmo', 'ēmā', 'ēmó']\n",
      "['2889', '尺寸', '尺寸', '5', '9.購物、商店', '13', '13', '[[\"尺寸\", [\"31455\"]]]'] :\t 尺寸 chícun vs. ['chǐcun', 'chǐcùn', 'chěcun', 'chěcùn']\n",
      "['3331', '古跡/古蹟', '古迹/古迹', '5', '5.交通、旅遊', '7', '26', '[]'] :\t 古蹟 gǔjī vs. ['gǔjì']\n",
      "['4063', '奇蹟', '奇迹', '5', '核心詞', '31', '24', '[[\"奇蹟\", [\"24507\"]]]'] :\t 奇蹟 qíjī vs. ['qíjì', 'jījì']\n",
      "['5329', '磅', '磅', '6', '', '12', '3', '[[\"磅\", [\"2839\", \"1146\"]]]'] :\t 磅 pāng vs. ['páng', 'bàng']\n",
      "['7155', '摟', '搂', '6', '', '9', '1', '[[\"摟\", [\"13652\", \"13656\", \"13677\"]]]'] :\t 摟 lóu vs. ['lǒu', 'lōu', 'lou']\n",
      "['7486', '鋪/舖', '铺/舖', '6', '', '32', '26', '[[\"鋪\", [\"3515\", \"3440\"]]]'] :\t 舖 pū vs. ['pù']\n",
      "['7542', '齊', '齐', '6', '', '33', '22', '[[\"齊\", [\"21691\", \"24545\", \"36690\"]]]'] :\t 齊 zī vs. ['qí']\n",
      "['7946', '事蹟', '事迹', '6', '', '15', '5', '[[\"事蹟\", [\"33653\"]]]'] :\t 事蹟 shìjī vs. ['shìjì', 'shijì']\n",
      "['8358', '蜿蜒', '蜿蜒', '6', '', '8', '7', '[[\"蜿蜒\", [\"43635\"]]]'] :\t 蜿蜒 wǎnyán vs. ['wānyán', 'wānyan']\n",
      "['8440', '無妨', '无妨', '6', '', '7', '1', '[[\"無妨\", [\"42716\"]]]'] :\t 無妨 wúfāng vs. ['wúfáng', 'mófáng']\n",
      "['9256', '忠告', '忠告', '6', '', '10', '2', '[[\"忠告\", [\"31207\"]]]'] :\t 忠告 zhōnggù vs. ['zhōnggao', 'zhōnggào']\n",
      "['9386', '縱', '纵', '6', '', '19', '3', '[[\"縱\", [\"37678\", \"37736\"]]]'] :\t 縱 zōng vs. ['zòng']\n",
      "['9387', '縱橫', '纵横', '6', '', '10', '5', '[[\"縱橫\", [\"37682\"]]]'] :\t 縱橫 zōnghéng vs. ['zònghèng', 'zònghéng']\n",
      "['11561', '牢靠', '牢靠', '7', '', '2', '1', '[[\"牢靠\", [\"13430\"]]]'] :\t 牢靠 láokao vs. ['láokào']\n",
      "['11583', '怔', '怔', '7', '', '4', '1', '[[\"怔\", [\"13923\", \"30226\"]]]'] :\t 怔 lèng vs. ['zhèng', 'zhēng']\n",
      "['12041', '澎湃', '澎湃', '7', '', '6', '3', '[[\"澎湃\", [\"2890\"]]]'] :\t 澎湃 pēngpài vs. ['péngpài']\n",
      "['12098', '平反', '平反', '7', '', '6', '1', '[[\"平反\", [\"3293\"]]]'] :\t 平反 píngfān vs. ['píngfǎn']\n",
      "['12134', '鋪陳/舖陳', '铺陈/舖陈', '7', '', '6', '1', '[[\"鋪陳\", [\"3446\"]]]'] :\t 舖陳 pūchén vs. ['pùchén']\n",
      "['12136', '鋪路/舖路', '铺路/舖路', '7', '', '4', '2', '[[\"鋪路\", [\"3442\"]]]'] :\t 舖路 pūlù vs. ['pùlù', 'pùlu']\n",
      "['12138', '鋪設/舖設', '铺设/舖设', '7', '', '5', '8', '[[\"鋪設\", [\"3447\"]]]'] :\t 舖設 pūshè vs. ['pùshè', 'pùshe']\n",
      "['12667', '視網膜', '视网膜', '7', '', '5', '3', '[]'] :\t 視網膜 shìwǎngmò vs. ['shìwǎngmó']\n",
      "['12886', '探頭', '探头', '7', '', '6', '1', '[[\"探頭\", [\"10547\"]]]'] :\t 探頭 tāntóu vs. ['tàntóu', 'tàntou']\n",
      "['14109', '震懾', '震慑', '7', '', '4', '2', '[[\"震懾\", [\"30078\"]]]'] :\t 震懾 zhènzhé vs. ['zhènshè']\n",
      "['14244', '皺眉頭/皺眉', '皱眉头/皱眉', '7', '', '9', '1', '[]'] :\t 皺眉頭 zhòuméi vs. ['zhòuméitóu', 'zhòuméitou']\n",
      "['14244', '皺眉頭/皺眉', '皱眉头/皱眉', '7', '', '9', '1', '[]'] :\t 皺眉 zhòuméitóu vs. ['zhòuméi']\n",
      "['14326', '自白', '自白', '7', '', '3', '0', '[[\"自白\", [\"36768\"]]]'] :\t 自白 zìbó vs. ['zibái', 'zibai', 'zìbái', 'zìbai']\n"
     ]
    }
   ],
   "source": [
    "# Check readings\n",
    "if os.path.exists('../cedict/syllables.csv'):\n",
    "    readings_mp = {'一': set(['yì','yí'])}\n",
    "    syll_df = pd.read_csv('../cedict/syllables.csv', dtype='str').fillna('')\n",
    "    for row in syll_df.itertuples():\n",
    "        readings_mp.setdefault(row.Traditional, set()).add(row.Pinyin.lower())\n",
    "        readings_mp.setdefault(row.Simplified, set()).add(row.Pinyin.lower())\n",
    "    readings_mp = {x: set([y.strip().lower() for y in readings_mp[x] if y.strip()]) for x in readings_mp}\n",
    "\n",
    "    def gen_readings(trad):\n",
    "        if trad == '':\n",
    "            yield ''\n",
    "        elif trad[0] not in readings_mp or ord(trad[0]) < 0x3E00:\n",
    "            yield from gen_readings(trad[1:])\n",
    "        else:\n",
    "            for x in readings_mp[trad[0]]:\n",
    "                for y in gen_readings(trad[1:]):\n",
    "                    yield x.lower() + (\"'\" if y and y[0] in 'aāáǎàeēéěèoōóǒò' else '') + y\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        variants = json.loads(row.Variants) if row.Variants else [[row.Traditional, row.Pinyin]]\n",
    "        for trad, pinyin in variants:\n",
    "            readings = list(gen_readings(trad))\n",
    "            if re.sub('', '', pinyin) not in readings:\n",
    "                print(list(row._asdict().values())[1:9], ':\\t', trad, pinyin, 'vs.', readings[:min(10, len(readings))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140a3f32-3ccc-48f6-9a0a-2371d9b1763c",
   "metadata": {},
   "source": [
    "## Merge with CEDICT and generate anki deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b57f33fc-8e25-4d6e-8405-57fd35745d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tbcl.csv', dtype='str').fillna('')\n",
    "cedict_df = pd.read_csv('../cedict/cedict.csv')\n",
    "cedict_idx_mp = cedict_df.assign(idx=cedict_df.index).groupby('Traditional').idx.apply(list)\n",
    "\n",
    "rows = []\n",
    "\n",
    "for row in df.fillna('').to_dict(orient='records'):\n",
    "    py = set([row['Pinyin']])\n",
    "    variants = json.loads(row['Variants']) if row['Variants'] else [[row['Traditional'], row['Pinyin']]]\n",
    "    if len(variants) == 1:\n",
    "        variants[0].append(row['Simplified'])\n",
    "    else:\n",
    "        variants = [(t,p,to_simplified(t)) for t,p in variants]\n",
    "        for var in variants:\n",
    "            py.add(var[1])\n",
    "\n",
    "    matches = cedict_idx_mp.get(row['Traditional'], [])\n",
    "    if len(matches) == 0:\n",
    "        for v in variants:\n",
    "            matches.extend(cedict_idx_mp.get(v[0], []))\n",
    "\n",
    "    flag = ''\n",
    "    if len(matches) == 0:\n",
    "        if row['MOE'] != '[]' and row['Level'] <= '5':\n",
    "            flag = 'missing'\n",
    "        row['CEDICT'] = ''\n",
    "    else:\n",
    "        ce_py = set([cedict_df.Pinyin[i].lower() for i in matches])\n",
    "        for v in variants:\n",
    "            if re.sub(' ', '', v[1]) not in ce_py:\n",
    "                #flag += ' py%s/%s' % (ce_py, v[1])\n",
    "                break\n",
    "\n",
    "        ce_s = set([cedict_df.Simplified[i] for i in matches])\n",
    "        for trad,p,simp in variants:\n",
    "            if trad in cedict_idx_mp and simp not in ce_s:\n",
    "                flag += ' simp%s/%s' % (ce_s, simp)\n",
    "                break\n",
    "\n",
    "        # Prioritize pronunciation matches\n",
    "        # TODO extract to cedict-lib + taiwan pr.\n",
    "        if len(matches) > 1:\n",
    "            m = [i for i in matches if cedict_df.Pinyin[i].lower() in py]\n",
    "            if len(m) > 0:\n",
    "                matches = m + [i for i in matches if i not in m]\n",
    "\n",
    "        defs = []\n",
    "        for i in matches:\n",
    "            py1 = list(py)[0] if len(py) == 1 else ''\n",
    "            defn = cedict_df.Definitions[i]\n",
    "            if row['Variants']:\n",
    "                defn = '%s [%s] %s' % (cedict_df.Traditional[i], cedict_df.Pinyin[i], defn)\n",
    "            elif cedict_df.Pinyin[i].lower() != py1:\n",
    "                defn = '[%s] %s' % (cedict_df.Pinyin[i], defn)\n",
    "            defn = re.sub(r'/CL:個\\|个\\[ge4\\]$', '', defn)\n",
    "            defs.append(defn)\n",
    "\n",
    "        row['CEDICT'] = '<br> '.join(defs)\n",
    "\n",
    "    row['Flag'] = flag\n",
    "    rows.append(row)\n",
    "\n",
    "merged_df = pd.DataFrame(rows).drop(columns=['Context', 'Zhuyin'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3994829-978e-4d2c-9779-71449ae11558",
   "metadata": {},
   "source": [
    "*Taiwan TBCL wordlist (Traditional)*\n",
    "\n",
    "TBCL (Taiwan Benchmarks for the Chinese Language) wordlist, 14425 words over 7 levels. Parsed from official excel sheets from [TBCL](https://coct.naer.edu.tw/TBCL/) website, including definitions/examples for about 1500 lower level words that they provide. CC-CEDICT definitions for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97300889-3880-45ed-abeb-e228f1f81460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 jovyan users 17426742 Nov  4 14:58 tbcl.apkg\n"
     ]
    }
   ],
   "source": [
    "import genanki, shutil\n",
    "\n",
    "df = merged_df.copy()\n",
    "df['Audio'] = ''\n",
    "\n",
    "!mkdir -p data/media\n",
    "!cp -f ../downloads/fonts/MoeStandardKai.ttf data/media/_MoeStandardKai.ttf\n",
    "\n",
    "cols = ['ID', 'Traditional', 'Simplified', 'Pinyin', 'Level', 'WritingFreq', 'SpeakingFreq', 'MOE',\n",
    "        'POS', 'Meaning', 'Compounds', 'Examples', 'CEDICT', 'Variants', 'Audio']\n",
    "\n",
    "model = genanki.Model(\n",
    "    1698579990,\n",
    "    'TBCL',\n",
    "    fields=[{'name': c} for c in cols],\n",
    "    templates=[{\n",
    "        'name': 'TBCL',\n",
    "        'qfmt': open('../dangdai/dangdai-qfmt.html').read().replace('{{ID}}', 'TBCL L{{Level}}'),\n",
    "        # TODO fix template\n",
    "        'afmt': '''{{FrontSide}}\n",
    "<hr id=answer>\n",
    "<div lang=\"en\"><span id=\"ddzw-pinyin\">{{Pinyin}}</span></div><br>\n",
    "<div lang=\"en\">{{#POS}}({{POS}}) {{/POS}}{{Meaning}}</div><br>\n",
    "<div lang=\"en\">{{CEDICT}}</div><br>\n",
    "<div>{{Compounds}}<br>{{Examples}}</div><br>\n",
    "<div>{{Audio}}</div>\n",
    "''' + re.sub('^.*<script>', '<script>', open('../dangdai/dangdai-afmt.html').read()).replace(\n",
    "            'if (pinyinEl && hanziEl)',\n",
    "            'if (pinyinEl && hanziEl {{#Variants}}&& false{{/Variants}})'),\n",
    "    }],\n",
    "    css=open('../dangdai/dangdai.css').read(),\n",
    ")\n",
    "\n",
    "deck = genanki.Deck(1698579991, name='tbcl')\n",
    "\n",
    "for row in df.reset_index().to_dict(orient='records'):\n",
    "    note = genanki.Note(\n",
    "        model=model,\n",
    "        fields=[row[c] for c in cols],\n",
    "        guid=genanki.guid_for('tbcl', row['ID']),\n",
    "        tags=['L%s' % row['Level'][0]],\n",
    "    )\n",
    "    deck.add_note(note)\n",
    "\n",
    "!rm -f tbcl.apkg\n",
    "genanki.Package(deck, media_files=glob.glob('data/media/*')).write_to_file('tbcl.apkg')\n",
    "!ls -l tbcl.apkg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
